{"meta":{"title":"Hexo","subtitle":"","description":"","author":"Jiacheng Chen","url":"https://jiachengc320.github.io","root":"/"},"pages":[{"title":"1","date":"2023-10-10T15:39:27.000Z","updated":"2023-10-10T15:41:52.996Z","comments":false,"path":"tags/index.html","permalink":"https://jiachengc320.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Stable diffusion详解","slug":"Stable-diffusion详解","date":"2024-12-18T15:00:11.000Z","updated":"2024-12-18T15:05:39.889Z","comments":true,"path":"2024/12/18/Stable-diffusion详解/","link":"","permalink":"https://jiachengc320.github.io/2024/12/18/Stable-diffusion%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"DDPM 正向过程 前向过程：给定真实图片样本$x_0 q(x) $ ,前向过程通过T次累计对其添加高斯噪声，每一步大小是由一系列的高斯分布方差的超参数\\(\\{ \\beta_t \\in (0,1)_{t=1} ^T \\}\\)控制。前向过程由于每个时刻t只与t-1时刻有关，所以可看作马尔科夫过程： \\[ q(x_t | x_{t-1})= \\mathcal{N}(x_t;\\sqrt{1-\\beta_t} x_{t-1},\\beta_t I ) \\] 整个前向过程是一个后验估计，被表示为：（根据联合概率密度+马尔科夫链） \\[ q(x_{1:T} |x_0) =\\prod _{t=1} ^T q(x_t|x_{t-1}) \\] \\(\\beta_t\\)为方差 \\(\\sqrt{1-\\beta_t}\\)为均值，\\(\\alpha_t=1-\\beta_t\\) 重参数技巧 从高斯分布$z (z;,^2 I) $ 采样\\(z\\) 可写成： \\[ z=\\mu_\\theta +\\sigma_\\theta \\odot \\epsilon , \\epsilon \\in \\mathcal{N}(0,I) \\] 因此\\(q(x_t | x_{t-1})= \\mathcal{N}(x_t;\\sqrt{1-\\beta_t} x_{t-1},\\beta_t I )\\) 可写成： \\[ x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1-\\alpha_t} \\epsilon_{t-1} \\] 可推出： \\[ q(x_t | x_0)=\\mathcal{N} (x_t; \\sqrt{\\bar \\alpha_t} x_0,(1-\\bar \\alpha_t )I ) \\] 其中\\(\\bar \\alpha_t = \\alpha_t \\alpha_{t-1} ... \\alpha_3 \\alpha_2 \\alpha_1\\) 逆向过程（去噪，生成） 属于Likelihood-based Model ，优化目标是最大化真实数据分布的似然估计\\(p_\\theta(x_0)\\) 其中\\(\\theta\\) 表示用用一个神经网络来学习 将每一步逆向过程建模成\\(p_\\theta(x_{t-1}|x_t)\\) 记作\\(p_\\theta(x_{t-1} |x_t) =\\mathcal{N}(x_{t-1};\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t) )\\) 整个逆向过程表示成一个联合概率分布$p_(x_{0:T})=p(x_T){t-1}^T p(x_{t-1}|x_t) $ 优化目标为： \\[ \\begin{align*} &amp; \\arg\\min\\limits_{\\theta} D_{KL} (q(x_{t-1}|x_t,x_0)||p_\\theta(x_{t-1} |x_t)) \\\\= &amp; \\arg\\min\\limits_{\\theta} \\frac{1}{2\\sigma_q^2(t)} \\frac{(1-\\alpha_t)^2}{(1-\\bar \\alpha_t) \\alpha_t } [||f_\\theta(x_t,t)-\\epsilon_t ||_2^2] \\end{align*} \\] 因此求： \\[ \\nabla _\\theta || \\epsilon-\\epsilon_\\theta (\\sqrt{\\bar \\alpha_t} x_0 + \\sqrt{1-\\bar \\alpha_t} \\epsilon,t ) ||^2 \\] 训练后，进行采样 采样则满足： \\[ x_{t-1} =\\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar \\alpha_t}} \\epsilon_\\theta(x_t,t)) +\\sigma_t z \\] 前向扩散： \\[ q(x_t | x_{t-1})= \\mathcal{N}(x_t;\\sqrt{1-\\beta_t} x_{t-1},\\beta_t I ) \\] 反向生成： \\[ q(x_{t-1}|x_t,x_0)=\\mathcal{N}(x_t,;\\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{(1-\\alpha_t)}{\\sqrt{(1-\\bar \\alpha_t)}} \\epsilon_t) ,\\beta_t \\frac{1-\\bar \\alpha_{t-1}}{1-\\bar \\alpha_t} I) \\] Stable Diffusion LDM=VAE+DDPM LDM是在语义空间做diffusion,DDPM是在像素空间做diffusion LDM有更多模态的融入：1. 类别融入 2. 文本融入 Diffusion Model模型的训练和推理非常耗时 语义空间为什么work？ 数字图像的大多数像素都用来描述细节 在像素空间做DM，存在大量计算冗余 如何控制图片的生成？ 类别控制 Classifier Guidance 单独训练一个分类器，采样是加入分类器的信息 训练一个什么样的分类器？ 输入输出是什么？ 采样时怎么加入分类信息？ Classifier Free 文本控制 Text Guidance Classifier Guidance 给定y的条件下，求\\(p(x_{t-1}|x_t)\\) $$ \\[\\begin{align*} p(x_{t-1}|x_t,y) &amp;=\\frac{p(x_t|x_{t-1},y) p(x_{t-1}|y) }{p(x_t|y)} \\\\ &amp;=\\frac{ p(x_t|x_{t-1})p(x_{t-1}|y) }{p(x_t|y)} \\end{align*}\\] $$ 为什么UNet中图像信息与Q矩阵结合 图像生成任务的目的是从噪声中生成图像，Q代表的是当前图像特征的上下文，因此在每个生成步骤中，图像特征会被逐步调整和改进，使得不同部分需要逐步去适应文本信息，生成符合文本描述的图像。 文本嵌入提供了如何生成图像的语义指导，图的上下文Q需要根据这个语义指导进行调整 交互核心：图像生成与文本引导 跨模态注意力的核心目的，是让模型通过结合文本描述的指导信息来生成图像，图像特征图Q是输入的上下文，文本嵌入K和V则提供了语义上的指引， image-20241209145028295 在进行Stable Diffusion模型训练时，VAE部分和CLIP部分是冻结的 stable diffusion 文生图 使用CLIP Text Encodear模型将输入的人类文本信息进行编码，输出特征矩阵； 输入文本信息，再用random函数生成一个高斯噪声矩阵，作为Latent Feature的替代输入到SD模型的图像优化模块中 将图像优化模块进行优化迭代后的Latent Feature输入到图像解码器中，将Latent Feature重建成像素级图 image-20241209151432905 Stable Diffusuion图生图 在输入文本信息进行编码的同时，将原图片通过图像编码器（VAE Encoder）生成Latent Feature(隐空间特征)作为输入 将上述信息输入到SD模型的\"图像优化模块中\" 将图像优化模块进行优化迭代后的Latent Feature输入到图像解码器中，将Latent Feature重建成像素级图 image-20241209152033425 文生图与图生图的核心都是图像优化模块，其输入都是文字+图片，输出都是一张经过优化后的图片；但温升图任务中图像优化模块的输入是一张随机生成的噪声图 图像优化模块 由U-Net网络和一个Schedule算法共同组成 U-Net网络负责预测噪声，不断优化生成过程，在预测噪声的同时不断注入文本语义信息 schedule算法对每次U-Net预测的噪声进行优化处理（动态调整预测的噪声，控制U-Net预测噪声的强度），从而统筹生成过程的进度 SD中，U-Net迭代优化部署大概是50或100 DDPM训练过程 image-20241209203835039 从数据集中随机选择一个训练样本，从K个噪声量级随机抽取一个时间t，并编码 通过$x_t = x_0 + $ 将高斯噪声添加到图片中得到 Noisy image,其中$(0,1),_t=1-_t ,t={i=1} ^t _i $ 将Noisy image和Time step embedding一起加入到UNet中 ResNetBlock模块 借助resnet的残差结构，让网络能够构建更深的同时，将Time Embedding信息嵌入模型 CrossAttention模块 接受ResNetBlock模块的输出和Clip text Encoder编码后的Context Embedding其不改变输入输出的尺寸，只在图片对应的位置上融合了语义信息 BasicTransformer Block模块在Cross Attention模块的基础上，增加了self Attention子模块和Feedforward子模块共同组成。 SelfAttention 可以将输入图像的不同部分进行交互，实现特征的整合和全局上下文的引入，让模型建立捕捉图像全局关系的能力，有助于模型理解不同位置不同像素之间的依赖关系，以更好地理解图像的语义。 Spatial Transformer模块 在BasicTransformer Block模块基础上，加入GroupNorm和两个卷积层组成Spatial Transformer模块 在生成式模型中，GroupNorm的效果一般会比BatchNorm更好，生成式模型通常比较复杂，因此需要更稳定和适应性强的归一化方法，SiLU在深层模型上的效果优于 ReLU。可以看做是平滑的ReLU激活函数 Stable Diffusion中U-Net的训练过程与损失函数 在我们进行Stable Diffusion模型训练时，VAE部分和CLIP部分都是冻结的，所以说官方在训练SD系列模型的时候，训练过程一般主要训练U-Net部分 \\[ L_{SD} = \\mathbb{E}_{x_0,\\epsilon \\sim \\mathcal{N}(0,I),t}[||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar \\alpha_t}x_0+\\sqrt{1-\\bar \\alpha_t}\\epsilon,t,c)||^2] \\]","categories":[],"tags":[]},{"title":"Diffusion Transformer详解","slug":"Diffusion_Transformer详解","date":"2024-12-15T07:10:35.000Z","updated":"2024-12-18T14:56:42.938Z","comments":true,"path":"2024/12/15/Diffusion_Transformer详解/","link":"","permalink":"https://jiachengc320.github.io/2024/12/15/Diffusion_Transformer%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Vit架构及介绍 vit是谷歌提出的把Transformer应用到图像分类的模型，论文的最核心结论是：当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果 当训练数据集不够大的时候，Vit通常比同等大小的Resnets要差一些，因为Transformer和CNN相比缺少归纳偏置，即一种先验知识。（CNN具有两种归纳偏置，1.局部性，图片上相邻区域具有相似的特征；2.平移不变性。CNN具有上述两种归纳偏置，具有先验信息，相对少的数据就可以学到一个比较好的模型） Vit的架构 Vit将输入图片分为多个patch(16x16)，再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同，但需要对图像进行分类，故而在输入序列中加入一个特殊的token。该token对应的输出即为最后的类别预测 vit架构图 步骤 patch embedding：如输入的图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，每张图片会生成196个patch，每个patch的维度为16x16x3，输入通过线性投影层后维度依然为196x768。这里需要加一个特殊的字符cls，则最终维度为197 x 768 。将视觉问题转化为了一个seq2seq问题。 positional encoding ：Vit需要加入位置编码。位置编码可理解成一张表，共N行，与输入序列长度相同(197)，每行代表一个向量，向量维度与输入序列embedding的维度相同（768）。编码的操作是sum LN/multi-head attention/LN 维度依然是197x768 MLP：将维度放大，再缩回去，197x768-&gt;197x3072-&gt;197x768。 一个Block之后维度保持不变，因此可以堆叠多个，最后将特殊字符cls对应的输出\\(z_L^0\\)作为encoder的最终输出。 （另一种做法：不加cls字符，对所有tokens的输出做一个平均） 关于positional encoding 1-D 位置编码， 1-9 2-D 位置编码， 11,12，... ,33 奇怪现象：1-D，2-D，不适用位置编码，性能非常接近，即使不使用位置编码，模型的性能损失也不是很大。 解释： Patch-Level Processing(最主要原因) VIT模型将图像分为多个小块，并将每个小块作为一个单独的输入。相比于处理像素级别的信息，这些小块之间的相对位置更容易被模型理解和利用。通过这些小块进行自注意力机制，能够捕捉到他们之间的空间关系 transformer的自注意力机制： Vit的Transformer 层具有强大的自注意力机制，可以在一个序列中的不同位置之间建立复杂的关联 模型的学习能力。 优点： 全局感知特性：ViT能够在全局范围内捕捉特征和上下文关系，源于它的自注意力机制，能够理解图像的全局上下文 数据效率：ViT在大规模数据集上表现优秀，这使得ViT可以更有效利用那些包含超大量样本的数据集 数据通用性：ViT借鉴了transformer模型，具有很强的迁移学习能力，更有利于后续的模态融合。 缺点： 需要大量数据：ViT要活的良好性能，需要大量训练数据，对于小规模数据集不如CNN 计算成本高：ViT的计算和内存需求比传统的CNN大，特别在高分辨率图像中，自注意力的计算复杂度会随着像素数量的增加而增加 可解释性差：尽管Transformer结构在很多任务上有很好的表现，但它的可解释性往往较差。这是因为ViT模型学到的全局信息和关系，可能并不直观或容易理解，这给模型的理解和优化带来了困难。 ViT适合在哪些场景下应用： 大规模图像识别：ViT在数据丰富的环境下表现尤其突出，因此适合需要在大规模数据集上进行复杂图像识别任务的场景，例如图像分类、物体识别等。 高清图像处理：利用它的全局感知和高分辺率处理能力，ViT适用于高清图像的分析，如医学成像、卫星图像分析等。 迁移学习任务：ViT的模型通用性使它在迁移学习场景中表现良好，可以应用于那些与预训练数据领域相近的任务上。 多模态学习：因为Transformer架构的灵活性，ViT可以与其他模式的Transformer（如用于文本或音频的Transformer）结合，进行多模态学习，比如图像-文本匹配等。 科学研究：在需要精确理解图像全局上下文的科学研究中，比如地理信息系统(GIS)、天体物理学等，ViT由于其全局感知特性也非常有用。 Dit 架构及介绍 《Scalable Diffusion Models with Transformers》 Patchify过程 图像的大小为\\(I \\times I \\times c\\) ，将图像Patchify，并经过Linear Embedding，变成T个d维的tokens，经过Linear Embedding，得到T个d维的tokens。 Patchify后，作者将标准的基于Vit频率的位置编码（sin-cosine）应用到输入的tokens上 作者在Dit design space里使用p=2,4,8，token T的数量由Patch大小p决定，满足\\(T=(I/p)^2\\) Dit Block设计 Patchify后，输入的tokens开始进入一系列的Transformer Block中，Diffusion Model会额外处理条件信息，如噪声时间步长t，标签c,自然语言text。 作者探索了4种不同类型的Transformer Block，以不同的方式处理条件输入，这些设计对标准Vit Block进行微小修改。 Dit 框架图 In-Context Conditioning 带条件输入的情况，该方法将时间步长t，类标签c作为两个额外的token附加到输入的序列中。 作者将其视为与图像的token无区别，允许Dit使用标准的Vit Block（无修改），经过最后一个Block后，删除条件token即可。 In-Context Conditioning带来的额外 GFLOPs 微不足道。 Cross-Attention Block 该方法将时间步长t和类标签c的Embedding连接成一个长度为2的Sequence，且与image token序列分开，该方法给Transformer Block添加一个Cross-Attention块。 Cross-Attention Block带来的额外GFLOPs开销大约15%。 Adaptive Layer Norm（adaLN）Block adaLN Block遵循GAN中的自适应归一化层，探索这个在扩散模型里是否适用。本文作者并未学习缩放和移位参数\\(\\gamma\\) 和\\(\\beta\\)，而是改用噪声时长t和类标签c得到。 Adaptive Layer Norm（adaLN）Block带来的额外的 GFLOPs 是最少的。 adaLN-Zero Block 有监督学习中，对于每个Block的第一个Batch Norm操作的缩放因子进行Zero-Initialization可以加速其大规模训练。基于U-Net的扩散模型使用类似的初始化策略，对每个Block的第一个卷积进行Zero-Initialization。本文作者做了一些改进：回归计算缩放参数\\(\\gamma\\)和移位参数\\(\\beta\\)外，还回归缩放系数\\(\\alpha\\)。 作者初始化MLP使其输出的缩放系数\\(\\alpha\\) 全部为0，使得Dit Block初始化为恒等函数（Identity Function）。 adaLN-Zero Block带来的额外的GFLOPs可忽略不计 Transformer Decoder 在最后一个Dit Block后，需要将image tokens的序列解码为输出噪声以及对角的协方差矩阵的预测结果，且这两个输出的形状都与原始空间输入一致。作者在该环节使用标准的线性解码器，将每个token线性解码为\\(p \\times p \\times 2C\\)的张量，其中\\(C\\)是空间输入中到Dit的通道数，最后将解码的tokens重新排列到其原始空间布局中，得到预测的噪声和协方差。 Dit 训练策略 训练设置 作者在ImageNet 数据集上训练class-conditional latent Dit模型，分辨率设为256x256和512x512，对最后的Linear Layer使用Zero Initialization,其余使用Vit的标准权重初始化方案。优化器使用AdamW 扩散模型配置 Dit其他组件，作者中Stable Diffusion预训练好的Variational AutoEncoder模型，下采样率是8，给定256x246x3的输入图片x，得到的编码结果z尺寸为32x32x4，从扩散模型中Sample出一个新的latent结果后，作者使用VAE decoder，将其解码回pixel:\\(x=D(z)\\) 代码解析 Dit 模块图241218214457127 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122class DiT(nn.Module): &quot;&quot;&quot; Diffusion model with a Transformer backbone. &quot;&quot;&quot; def __init__( self, input_size=32, patch_size=2, in_channels=4, hidden_size=1152, depth=28, num_heads=16, mlp_ratio=4.0, class_dropout_prob=0.1, num_classes=1000, learn_sigma=True, ): super().__init__() self.learn_sigma = learn_sigma self.in_channels = in_channels self.out_channels = in_channels * 2 if learn_sigma else in_channels self.patch_size = patch_size self.num_heads = num_heads self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True) self.t_embedder = TimestepEmbedder(hidden_size) self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob) num_patches = self.x_embedder.num_patches # Will use fixed sin-cos embedding: self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False) self.blocks = nn.ModuleList([ DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth) ]) self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels) self.initialize_weights() def initialize_weights(self): # Initialize transformer layers: def _basic_init(module): if isinstance(module, nn.Linear): torch.nn.init.xavier_uniform_(module.weight) if module.bias is not None: nn.init.constant_(module.bias, 0) self.apply(_basic_init) # Initialize (and freeze) pos_embed by sin-cos embedding: pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5)) self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0)) # Initialize patch_embed like nn.Linear (instead of nn.Conv2d): w = self.x_embedder.proj.weight.data nn.init.xavier_uniform_(w.view([w.shape[0], -1])) nn.init.constant_(self.x_embedder.proj.bias, 0) # Initialize label embedding table: nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02) # Initialize timestep embedding MLP: nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02) nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02) # Zero-out adaLN modulation layers in DiT blocks: for block in self.blocks: nn.init.constant_(block.adaLN_modulation[-1].weight, 0) nn.init.constant_(block.adaLN_modulation[-1].bias, 0) # Zero-out output layers: nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0) nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0) nn.init.constant_(self.final_layer.linear.weight, 0) nn.init.constant_(self.final_layer.linear.bias, 0) def unpatchify(self, x): &quot;&quot;&quot; x: (N, T, patch_size**2 * C) imgs: (N, H, W, C) &quot;&quot;&quot; c = self.out_channels p = self.x_embedder.patch_size[0] h = w = int(x.shape[1] ** 0.5) assert h * w == x.shape[1] x = x.reshape(shape=(x.shape[0], h, w, p, p, c)) x = torch.einsum(&#x27;nhwpqc-&gt;nchpwq&#x27;, x) imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p)) return imgs def forward(self, x, t, y): &quot;&quot;&quot; Forward pass of DiT. x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images) t: (N,) tensor of diffusion timesteps y: (N,) tensor of class labels &quot;&quot;&quot; x = self.x_embedder(x) + self.pos_embed # (N, T, D), where T = H * W / patch_size ** 2 t = self.t_embedder(t) # (N, D) y = self.y_embedder(y, self.training) # (N, D) c = t + y # (N, D) for block in self.blocks: x = block(x, c) # (N, T, D) x = self.final_layer(x, c) # (N, T, patch_size ** 2 * out_channels) x = self.unpatchify(x) # (N, out_channels, H, W) return x def forward_with_cfg(self, x, t, y, cfg_scale): &quot;&quot;&quot; Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance. &quot;&quot;&quot; # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb half = x[: len(x) // 2] combined = torch.cat([half, half], dim=0) model_out = self.forward(combined, t, y) # For exact reproducibility reasons, we apply classifier-free guidance on only # three channels by default. The standard approach to cfg applies it to all channels. # This can be done by uncommenting the following line and commenting-out the line following that. # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:] eps, rest = model_out[:, :3], model_out[:, 3:] cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0) half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps) eps = torch.cat([half_eps, half_eps], dim=0) return torch.cat([eps, rest], dim=1) 如上图所示，x（images or latent representations of images）先经过PatchEmbed，与位置编码相加得到；y的embedding与t的embedding直接相加得到c，此后x与c经过若干个Dit Block。 Dit Block图241218214935006 12345678910111213141516171819202122class DiTBlock(nn.Module): &quot;&quot;&quot; A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning. &quot;&quot;&quot; def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs): super().__init__() self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6) self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs) self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6) mlp_hidden_dim = int(hidden_size * mlp_ratio) approx_gelu = lambda: nn.GELU(approximate=&quot;tanh&quot;) self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0) self.adaLN_modulation = nn.Sequential( nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size, bias=True) ) def forward(self, x, c): shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1) x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa)) x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp)) return x adaLN-Zero Block 是 adaptive LayerNorm-Zero Block 的简称。它是在传统的 Transformer 模块中引入了自适应的层归一化（Layer Normalization）机制，通过将条件信息（如扩散步数、标签等）动态地融入网络参数中，来提高生成模型的效果。它特别结合了 AdaLN 和 Zero Initialization，使得网络可以更好地适应不同的条件输入。 在传统 Layer Normalization 的基础上，AdaLN 使用外部条件（例如时间步 t）来动态调整 LayerNorm 的尺度和偏移参数。具体表达为： \\[ AdaLN(h;\\gamma(t),\\beta(t))=\\gamma(t) \\cdot \\frac{h-\\mu}{\\sigma} +\\beta(t) \\] 1234567891011121314def modulate(x, shift, scale): return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)def forward(self, x, c): shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1) x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa)) x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp)) return x self.adaLN_modulation = nn.Sequential( nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size, bias=True) ) 这里的6组可学习的参数由self.adaLN_modulation线性层，再通过chunk(6, dim=1)得到 这里的Multi-Head Self-Attention和Pointwise Feedforward分别为： 123self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0) 它称为 Pointwise，是因为它的操作不考虑不同位置之间的交互，仅在单个位置的向量上进行变换。 Pointwise Feedforward Network 是 Transformer 架构中常见的模块，它对输入序列中的每个位置（token）的特征向量 独立 进行相同的前向传播运算（升维 -&gt; 激活 -&gt; 降维） 普通的 Feedforward Network 是标准的全连接神经网络，它将整个输入向量映射到输出向量，通过非线性激活函数引入非线性特性。 Zero Initialization Zero Initialization 的关键思想是在模块开始训练时，将残差块的输出初始化为零，以确保初始时模型的输入和输出保持一致。这种做法有两个好处： 稳定训练：避免训练初期网络过早出现大幅度的变化。 逐渐学习特征：残差分支在训练过程中逐渐学习有用的特征，从零开始优化。 在实现上，网络的残差连接的输出会被一个动态缩放因子初始化为零。 12345678910111213def initialize_weights(self): ... # Zero-out adaLN modulation layers in DiT blocks: for block in self.blocks: nn.init.constant_(block.adaLN_modulation[-1].weight, 0) nn.init.constant_(block.adaLN_modulation[-1].bias, 0) # Zero-out output layers: nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0) nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0) nn.init.constant_(self.final_layer.linear.weight, 0) nn.init.constant_(self.final_layer.linear.bias, 0) Final Layer ——Transformer Decoder 最后一个Dit Block后，需要将image tokens的序列解码为输出噪声以及对角的协方差矩阵的预测结果，且这两个输出的形状都与原始空间输入一致。作者在该环节使用标准的线性解码器，将每个token线性解码为p × p× 2C的张量，其中C是空间输入中到Dit的通道 数，最后将解码的tokens重新排列到其原始空间布局中，得到预测的噪声和协方差。 123456789101112131415161718class FinalLayer(nn.Module): &quot;&quot;&quot; The final layer of DiT. &quot;&quot;&quot; def __init__(self, hidden_size, patch_size, out_channels): super().__init__() self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6) self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True) self.adaLN_modulation = nn.Sequential( nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True) ) def forward(self, x, c): shift, scale = self.adaLN_modulation(c).chunk(2, dim=1) x = modulate(self.norm_final(x), shift, scale) x = self.linear(x) return x ##(N, T, patch_size ** 2 * out_channels) 1234567891011121314def unpatchify(self, x): &quot;&quot;&quot; x: (N, T, patch_size**2 * C) imgs: (N, H, W, C) &quot;&quot;&quot; c = self.out_channels p = self.x_embedder.patch_size[0] h = w = int(x.shape[1] ** 0.5) assert h * w == x.shape[1] x = x.reshape(shape=(x.shape[0], h, w, p, p, c)) x = torch.einsum(&#x27;nhwpqc-&gt;nchpwq&#x27;, x) imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p)) return imgs 1self.out_channels = in_channels * 2 if learn_sigma else in_channels 说明了如果预测协方差矩阵的化，out_channels为2c，即第一部分c为噪声，第二部分c为预测的协方差矩阵","categories":[],"tags":[]},{"title":"大模型无监督训练","slug":"大模型无监督训练","date":"2024-12-03T14:25:10.000Z","updated":"2024-12-03T16:20:16.337Z","comments":true,"path":"2024/12/03/大模型无监督训练/","link":"","permalink":"https://jiachengc320.github.io/2024/12/03/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83/","excerpt":"","text":"无监督训练是机器学习的一种方式，它不依赖于带标签的数据来训练模型，而是通过从未标注的数据中发现潜在的结构或模式。对于语言模型，无监督学习通常意味着让模型通过大量的文本数据自我学习，从而能够理解语言的结构、上下文、语法规则。 无监督训练方法 自编码器：通过将输入映射到潜在空间，再从潜在空间恢复回输入来训练，学习数据的低维表示 聚类：聚类算法视图将数据集分城若干类别或簇，模型通过这些簇来发现数据中的结构 生成模型：生成式模型视图学习数据的分布，并从中生成新的数据 无监督训练在LLM中的应用 LLM通常采用的无监督训练方式是通过自回归训练（autoregressive training）或者掩码语言建模（masked language modeling）来训练。 自回归训练语言建模：模型的目标是预测一个句子中的每个单词的下一个单词，对于大量未标注的文本数据，模型通过这种方式逐步学习语言的结构和上下文信息 掩码语言建模：通常用于Bert类模型，句子随机掩盖掉一些词，模型的任务是根据上下文信息预测被掩盖的词。例如，在句子“The cat sat on the [MASK]”中，模型需要根据上下文预测出被掩盖的词“mat” 2）从未标注的文本中学习表示 无监督训练的另一个关键应用是让模型通过海量的文本数据学习到词语的表示，如词嵌入是通过无监督方法生成的，它能够将单词映射到高维空间，使得语义相近的词在该空间中更为接近。 3）迁移学习 无监督训练往往是LLM的预训练阶段，而在实际应用时，模型可能会根据特定任务进行微调。 无监督训练在大规模语言模型（LLM）中的应用是至关重要的，它帮助模型从大量未标注的文本中学习语言的结构、上下文以及潜在的语义关系。通过无监督学习，LLM能够掌握丰富的语言知识，并为下游任务提供强大的支持。","categories":[],"tags":[]},{"title":"大模型微调方式","slug":"大模型微调方式","date":"2024-12-02T09:45:27.000Z","updated":"2024-12-02T09:49:43.384Z","comments":true,"path":"2024/12/02/大模型微调方式/","link":"","permalink":"https://jiachengc320.github.io/2024/12/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F/","excerpt":"","text":"全量微调 Full Fine-tuning 原理：在预训练的大型模型基础上，对模型的所有层和参数进行调整，使其适应特定任务。在这个过程中，模型会根据特定任务的数据重新学习和更新所有的权重参数，以达到更好地完成该任务的目的。 优点：可以充分利用预训练模型的通用特征，较好地适应特定任务。 缺点：计算成本高 基于适配器（Adapter）的微调： 原理：在预训练模型的每一层或某些层中添加适配器模块，微调时冻结预训练模型主体。由适配器模块学习特定下游任务的知识。 适配器由两个前馈子层组成，第一个将模型的输出作为输入，投影到较小维度，第二个将其还原到原始输入维度作为输出 优点：只需要训练少量的特定于任务的参数，降低了训练的计算成本和存储需求，预训练模型的主体被冻结，保留预训练模型的大部分知识，能快速适应新的任务。 缺点：增加了模型的复杂性，可能影响模型的推理速度，且适配器模块的设计要求较高 基于低秩适应（LORA）的微调 Low-Rank Adaptation of Large Language Models 原理：冻结预训练模型的矩阵参数，并引入额外的低秩矩阵代替模型权重的变化 image-20241122143906738 \\[ h=W_0 x+\\Delta Wx=W_0 x+BAx \\] 学习的是BA矩阵，来量化权重的差异\\(\\Delta W\\) P-Tuning 为了解决人工设计的Prompt问题： GPT Understands,TOO 大模型的Prompt构造方式严重影响下游任务的效果，如GPT3采用人工构造的模板做上下文学习，模板的变化较敏感，加减词或变动位置会造成较大变化 自动化搜索模板工作成本较高，离散化token搜索出的结果可能并不最优，性能不稳定 P-Tuning将提示Prompt转化为可学习的Embedding层，并使用MLP+LSTM的方式对Prompt Embedding进行一层处理。 P-Tuning 设计了一种连续可微的virtual token，将Prompt转换为可学习的Embedding层，使用MLP+LSTM的方式对Prompt Embedding进行一层处理。 经过预训练的LM的词嵌入变得高度离散，若随机初始化virtual token，容易优化到局部最优，作者通过实验发现用一个prompt encoder编码收敛更快，效果更好。\"virtual token\"指的是一种用于微调语言模型的特殊标记。 image-20241122155804651","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2023-10-10T13:31:50.872Z","updated":"2024-11-21T13:49:24.819Z","comments":true,"path":"2023/10/10/hello-world/","link":"","permalink":"https://jiachengc320.github.io/2023/10/10/hello-world/","excerpt":"","text":"graph TD; A-->B; A-->C; B-->D; C-->D; Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}