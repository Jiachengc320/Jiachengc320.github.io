{"meta":{"title":"Hexo","subtitle":"","description":"","author":"Jiacheng Chen","url":"https://github.com/Jiachengc320/Jiachengc320.github.io","root":"/Jiachengc320/Jiachengc320.github.io/"},"pages":[{"title":"1","date":"2023-10-10T15:39:27.000Z","updated":"2023-10-10T15:41:52.996Z","comments":false,"path":"tags/index.html","permalink":"https://github.com/Jiachengc320/Jiachengc320.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"大模型微调方式","date":"2024-11-22T08:00:45.468Z","updated":"2024-11-22T07:59:39.495Z","comments":true,"path":"2024/11/22/大模型微调方式/","link":"","permalink":"https://github.com/Jiachengc320/Jiachengc320.github.io/2024/11/22/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F/","excerpt":"","text":"全量微调 Full Fine-tuning 原理：在预训练的大型模型基础上，对模型的所有层和参数进行调整，使其适应特定任务。在这个过程中，模型会根据特定任务的数据重新学习和更新所有的权重参数，以达到更好地完成该任务的目的。 优点：可以充分利用预训练模型的通用特征，较好地适应特定任务。 缺点：计算成本高 基于适配器（Adapter）的微调： 原理：在预训练模型的每一层或某些层中添加适配器模块，微调时冻结预训练模型主体。由适配器模块学习特定下游任务的知识。 适配器由两个前馈子层组成，第一个将模型的输出作为输入，投影到较小维度，第二个将其还原到原始输入维度作为输出 优点：只需要训练少量的特定于任务的参数，降低了训练的计算成本和存储需求，预训练模型的主体被冻结，保留预训练模型的大部分知识，能快速适应新的任务。 缺点：增加了模型的复杂性，可能影响模型的推理速度，且适配器模块的设计要求较高 基于低秩适应（LORA）的微调 Low-Rank Adaptation of Large Language Models 原理：冻结预训练模型的矩阵参数，并引入额外的低秩矩阵代替模型权重的变化 \\[ h=W_0 x+\\Delta Wx=W_0 x+BAx \\] 学习的是BA矩阵，来量化权重的差异\\(\\Delta W\\) P-Tuning 为了解决人工设计的Prompt问题： GPT Understands,TOO 大模型的Prompt构造方式严重影响下游任务的效果，如GPT3采用人工构造的模板做上下文学习，模板的变化较敏感，加减词或变动位置会造成较大变化 自动化搜索模板工作成本较高，离散化token搜索出的结果可能并不最优，性能不稳定 P-Tuning将提示Prompt转化为可学习的Embedding层，并使用MLP+LSTM的方式对Prompt Embedding进行一层处理。 P-Tuning 设计了一种连续可微的virtual token，将Prompt转换为可学习的Embedding层，使用MLP+LSTM的方式对Prompt Embedding进行一层处理。 经过预训练的LM的词嵌入变得高度离散，若随机初始化virtual token，容易优化到局部最优，作者通过实验发现用一个prompt encoder编码收敛更快，效果更好。\"virtual token\"指的是一种用于微调语言模型的特殊标记 image-20241122155804651","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2023-10-10T13:31:50.872Z","updated":"2024-11-21T13:49:24.819Z","comments":true,"path":"2023/10/10/hello-world/","link":"","permalink":"https://github.com/Jiachengc320/Jiachengc320.github.io/2023/10/10/hello-world/","excerpt":"","text":"graph TD; A-->B; A-->C; B-->D; C-->D; Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}