

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Jiacheng Chen">
  <meta name="keywords" content="">
  
    <meta name="description" content="Vit架构及介绍 vit是谷歌提出的把Transformer应用到图像分类的模型，论文的最核心结论是：当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果 当训练数据集不够大的时候，Vit通常比同等大小的Resnets要差一些，因为Transformer和CNN相比缺少归纳偏置，即一种先验知识。（CNN">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Transformer详解">
<meta property="og:url" content="https://jiachengc320.github.io/2024/12/15/Diffusion_Transformer%E8%AF%A6%E8%A7%A3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Vit架构及介绍 vit是谷歌提出的把Transformer应用到图像分类的模型，论文的最核心结论是：当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果 当训练数据集不够大的时候，Vit通常比同等大小的Resnets要差一些，因为Transformer和CNN相比缺少归纳偏置，即一种先验知识。（CNN">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218225601439.png">
<meta property="og:image" content="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-5efe86e40ab87a05d33a2e8c75539f72_r.jpg">
<meta property="og:image" content="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218214457127.png">
<meta property="og:image" content="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218214935006.png">
<meta property="article:published_time" content="2024-12-15T07:10:35.000Z">
<meta property="article:modified_time" content="2024-12-18T14:56:42.938Z">
<meta property="article:author" content="Jiacheng Chen">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218225601439.png">
  
  
  
  <title>Diffusion Transformer详解 - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jiachengc320.github.io","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"g5HIusnoZkA0lmxRdHOC27Y1-MdYXbMMI","app_key":"yjfiVO4J8EuiOJCD2DwibypK","path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Fluid</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Diffusion Transformer详解"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-12-15 15:10" pubdate>
          December 15, 2024 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          12k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          103 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Diffusion Transformer详解</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="vit架构及介绍">Vit架构及介绍</h1>
<p>vit是谷歌提出的把Transformer应用到图像分类的模型，论文的最核心结论是：<strong>当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果</strong></p>
<p>当训练数据集不够大的时候，Vit通常比同等大小的Resnets要差一些，因为Transformer和CNN相比缺少归纳偏置，即一种先验知识。（CNN具有两种归纳偏置，1.局部性，图片上相邻区域具有相似的特征；2.平移不变性。CNN具有上述两种归纳偏置，具有先验信息，相对少的数据就可以学到一个比较好的模型）</p>
<h2 id="vit的架构">Vit的架构</h2>
<p>Vit将输入图片分为多个patch(16x16)，再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同，但需要对图像进行分类，故而在输入序列中加入一个特殊的token。该token对应的输出即为最后的类别预测</p>
<figure>
<img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218225601439.png" srcset="/img/loading.gif" lazyload title="vit架构图" alt="vit架构图" /><figcaption aria-hidden="true">vit架构图</figcaption>
</figure>
<h2 id="步骤">步骤</h2>
<ul>
<li><p>patch embedding：如输入的图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，每张图片会生成196个patch，每个patch的维度为16x16x3，输入通过线性投影层后维度依然为196x768。<strong>这里需要加一个特殊的字符cls</strong>，则最终维度为<strong>197 x 768</strong> 。将视觉问题转化为了一个seq2seq问题。</p></li>
<li><p>positional encoding ：Vit需要加入位置编码。位置编码可理解成一张表，共N行，与输入序列长度相同(197)，每行代表一个向量，向量维度与输入序列embedding的维度相同（768）。编码的操作是sum</p></li>
<li><p>LN/multi-head attention/LN 维度依然是197x768</p></li>
<li><p>MLP：将维度放大，再缩回去，197x768-&gt;197x3072-&gt;197x768。</p>
<p>一个Block之后维度保持不变，因此可以堆叠多个，最后将特殊字符cls对应的输出<span class="math inline">\(z_L^0\)</span>作为encoder的最终输出。</p>
<p>（另一种做法：不加cls字符，对所有tokens的输出做一个平均）</p></li>
</ul>
<h2 id="关于positional-encoding">关于positional encoding</h2>
<ul>
<li>1-D 位置编码， 1-9</li>
<li>2-D 位置编码， 11,12，... ,33</li>
</ul>
<p><strong>奇怪现象：1-D，2-D，不适用位置编码，性能非常接近，即使不使用位置编码，模型的性能损失也不是很大。</strong></p>
<p>解释：</p>
<ol type="1">
<li><p>Patch-Level Processing(最主要原因)</p>
<p>VIT模型将图像分为多个小块，并将每个小块作为一个单独的输入。相比于处理像素级别的信息，这些小块之间的相对位置更容易被模型理解和利用。通过这些小块进行自注意力机制，能够捕捉到他们之间的空间关系</p></li>
<li><p>transformer的自注意力机制：</p></li>
</ol>
<p>Vit的Transformer 层具有强大的自注意力机制，可以在一个序列中的不同位置之间建立复杂的关联</p>
<ol start="3" type="1">
<li>模型的学习能力。</li>
</ol>
<p>优点：</p>
<ol type="1">
<li>全局感知特性：ViT能够在全局范围内捕捉特征和上下文关系，源于它的自注意力机制，能够理解图像的全局上下文</li>
<li>数据效率：ViT在大规模数据集上表现优秀，这使得ViT可以更有效利用那些包含超大量样本的数据集</li>
<li>数据通用性：ViT借鉴了transformer模型，具有很强的迁移学习能力，更有利于后续的模态融合。</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>需要大量数据：ViT要活的良好性能，需要大量训练数据，对于小规模数据集不如CNN</li>
<li>计算成本高：ViT的计算和内存需求比传统的CNN大，特别在高分辨率图像中，自注意力的计算复杂度会随着像素数量的增加而增加</li>
<li>可解释性差：尽管Transformer结构在很多任务上有很好的表现，但它的可解释性往往较差。这是因为ViT模型学到的全局信息和关系，可能并不直观或容易理解，这给模型的理解和优化带来了困难。</li>
</ol>
<p><strong>ViT适合在哪些场景下应用：</strong></p>
<ol type="1">
<li><strong>大规模图像识别</strong>：ViT在数据丰富的环境下表现尤其突出，因此适合需要在大规模数据集上进行复杂图像识别任务的场景，例如图像分类、物体识别等。</li>
<li><strong>高清图像处理</strong>：利用它的全局感知和高分辺率处理能力，ViT适用于高清图像的分析，如医学成像、卫星图像分析等。</li>
<li><strong>迁移学习任务</strong>：ViT的模型通用性使它在迁移学习场景中表现良好，可以应用于那些与预训练数据领域相近的任务上。</li>
<li><strong>多模态学习</strong>：因为Transformer架构的灵活性，ViT可以与其他模式的Transformer（如用于文本或音频的Transformer）结合，进行多模态学习，比如图像-文本匹配等。</li>
<li><strong>科学研究</strong>：在需要精确理解图像全局上下文的科学研究中，比如地理信息系统(GIS)、天体物理学等，ViT由于其全局感知特性也非常有用。</li>
</ol>
<h1 id="dit-架构及介绍">Dit 架构及介绍</h1>
<p>《Scalable Diffusion Models with Transformers》</p>
<h2 id="patchify过程">Patchify过程</h2>
<ul>
<li><p>图像的大小为<span class="math inline">\(I \times I \times c\)</span> ，将图像Patchify，并经过Linear Embedding，变成T个d维的tokens，经过Linear Embedding，得到T个d维的tokens。</p></li>
<li><p>Patchify后，作者将标准的基于Vit频率的位置编码（sin-cosine）应用到输入的tokens上</p>
<p>作者在Dit design space里使用p=2,4,8，token T的数量由Patch大小p决定，满足<span class="math inline">\(T=(I/p)^2\)</span></p></li>
</ul>
<h2 id="dit-block设计">Dit Block设计</h2>
<p>Patchify后，输入的tokens开始进入一系列的Transformer Block中，Diffusion Model会额外处理条件信息，如噪声时间步长t，标签c,自然语言text。</p>
<p>作者探索了4种不同类型的Transformer Block，以不同的方式处理条件输入，这些设计对标准Vit Block进行微小修改。</p>
<figure>
<img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-5efe86e40ab87a05d33a2e8c75539f72_r.jpg" srcset="/img/loading.gif" lazyload title="Dit 框架图" alt="Dit 框架图" /><figcaption aria-hidden="true">Dit 框架图</figcaption>
</figure>
<ul>
<li>In-Context Conditioning</li>
</ul>
<p>带条件输入的情况，该方法将时间步长t，类标签c作为两个额外的token附加到输入的序列中。</p>
<p>作者将其视为与图像的token无区别，允许Dit使用标准的Vit Block（无修改），经过最后一个Block后，删除条件token即可。</p>
<p>In-Context Conditioning带来的额外 GFLOPs 微不足道。</p>
<ul>
<li>Cross-Attention Block</li>
</ul>
<p>该方法将时间步长t和类标签c的Embedding连接成一个长度为2的Sequence，且与image token序列分开，该方法给Transformer Block添加一个Cross-Attention块。</p>
<p>Cross-Attention Block带来的额外GFLOPs开销大约15%。</p>
<ul>
<li>Adaptive Layer Norm（adaLN）Block</li>
</ul>
<p>adaLN Block遵循GAN中的自适应归一化层，探索这个在扩散模型里是否适用。本文作者并未学习缩放和移位参数<span class="math inline">\(\gamma\)</span> 和<span class="math inline">\(\beta\)</span>，而是改用噪声时长t和类标签c得到。</p>
<p>Adaptive Layer Norm（adaLN）Block带来的额外的 GFLOPs 是最少的。</p>
<ul>
<li>adaLN-Zero Block</li>
</ul>
<p>有监督学习中，对于每个Block的第一个Batch Norm操作的缩放因子进行Zero-Initialization可以加速其大规模训练。基于U-Net的扩散模型使用类似的初始化策略，对每个Block的第一个卷积进行Zero-Initialization。本文作者做了一些改进：回归计算缩放参数<span class="math inline">\(\gamma\)</span>和移位参数<span class="math inline">\(\beta\)</span>外，还回归缩放系数<span class="math inline">\(\alpha\)</span>。</p>
<p>作者初始化MLP使其输出的缩放系数<span class="math inline">\(\alpha\)</span> 全部为0，使得Dit Block初始化为恒等函数（Identity Function）。</p>
<p>adaLN-Zero Block带来的额外的GFLOPs可忽略不计</p>
<h2 id="transformer-decoder">Transformer Decoder</h2>
<p>在最后一个Dit Block后，需要将image tokens的序列解码为输出噪声以及对角的协方差矩阵的预测结果，且这两个输出的形状都与原始空间输入一致。作者在该环节使用标准的线性解码器，将每个token线性解码为<span class="math inline">\(p \times p \times 2C\)</span>的张量，其中<span class="math inline">\(C\)</span>是空间输入中到Dit的通道数，最后将解码的tokens重新排列到其原始空间布局中，得到预测的噪声和协方差。</p>
<h2 id="dit-训练策略">Dit 训练策略</h2>
<h3 id="训练设置">训练设置</h3>
<p>作者在ImageNet 数据集上训练class-conditional latent Dit模型，分辨率设为256x256和512x512，对最后的Linear Layer使用Zero Initialization,其余使用Vit的标准权重初始化方案。优化器使用AdamW</p>
<h3 id="扩散模型配置">扩散模型配置</h3>
<p>Dit其他组件，作者中Stable Diffusion预训练好的Variational AutoEncoder模型，下采样率是8，给定256x246x3的输入图片x，得到的编码结果z尺寸为32x32x4，从扩散模型中Sample出一个新的latent结果后，作者使用VAE decoder，将其解码回pixel:<span class="math inline">\(x=D(z)\)</span></p>
<h2 id="代码解析">代码解析</h2>
<figure>
<img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218214457127.png" srcset="/img/loading.gif" lazyload title="Dit 模块图" alt="Dit 模块图241218214457127" /><figcaption aria-hidden="true">Dit 模块图241218214457127</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiT</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Diffusion model with a Transformer backbone.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_size=<span class="hljs-number">32</span>,</span><br><span class="hljs-params">        patch_size=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">        in_channels=<span class="hljs-number">4</span>,</span><br><span class="hljs-params">        hidden_size=<span class="hljs-number">1152</span>,</span><br><span class="hljs-params">        depth=<span class="hljs-number">28</span>,</span><br><span class="hljs-params">        num_heads=<span class="hljs-number">16</span>,</span><br><span class="hljs-params">        mlp_ratio=<span class="hljs-number">4.0</span>,</span><br><span class="hljs-params">        class_dropout_prob=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">        num_classes=<span class="hljs-number">1000</span>,</span><br><span class="hljs-params">        learn_sigma=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.learn_sigma = learn_sigma<br>        self.in_channels = in_channels<br>        self.out_channels = in_channels * <span class="hljs-number">2</span> <span class="hljs-keyword">if</span> learn_sigma <span class="hljs-keyword">else</span> in_channels<br>        self.patch_size = patch_size<br>        self.num_heads = num_heads<br><br>        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=<span class="hljs-literal">True</span>)<br>        self.t_embedder = TimestepEmbedder(hidden_size)<br>        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)<br>        num_patches = self.x_embedder.num_patches<br>        <span class="hljs-comment"># Will use fixed sin-cos embedding:</span><br>        self.pos_embed = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>, num_patches, hidden_size), requires_grad=<span class="hljs-literal">False</span>)<br><br>        self.blocks = nn.ModuleList([<br>            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(depth)<br>        ])<br>        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)<br>        self.initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># Initialize transformer layers:</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_basic_init</span>(<span class="hljs-params">module</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>                torch.nn.init.xavier_uniform_(module.weight)<br>                <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    nn.init.constant_(module.bias, <span class="hljs-number">0</span>)<br>        self.apply(_basic_init)<br><br>        <span class="hljs-comment"># Initialize (and freeze) pos_embed by sin-cos embedding:</span><br>        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-<span class="hljs-number">1</span>], <span class="hljs-built_in">int</span>(self.x_embedder.num_patches ** <span class="hljs-number">0.5</span>))<br>        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).<span class="hljs-built_in">float</span>().unsqueeze(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># Initialize patch_embed like nn.Linear (instead of nn.Conv2d):</span><br>        w = self.x_embedder.proj.weight.data<br>        nn.init.xavier_uniform_(w.view([w.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>]))<br>        nn.init.constant_(self.x_embedder.proj.bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Initialize label embedding table:</span><br>        nn.init.normal_(self.y_embedder.embedding_table.weight, std=<span class="hljs-number">0.02</span>)<br><br>        <span class="hljs-comment"># Initialize timestep embedding MLP:</span><br>        nn.init.normal_(self.t_embedder.mlp[<span class="hljs-number">0</span>].weight, std=<span class="hljs-number">0.02</span>)<br>        nn.init.normal_(self.t_embedder.mlp[<span class="hljs-number">2</span>].weight, std=<span class="hljs-number">0.02</span>)<br><br>        <span class="hljs-comment"># Zero-out adaLN modulation layers in DiT blocks:</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Zero-out output layers:</span><br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.bias, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unpatchify</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        x: (N, T, patch_size**2 * C)</span><br><span class="hljs-string">        imgs: (N, H, W, C)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        c = self.out_channels<br>        p = self.x_embedder.patch_size[<span class="hljs-number">0</span>]<br>        h = w = <span class="hljs-built_in">int</span>(x.shape[<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>)<br>        <span class="hljs-keyword">assert</span> h * w == x.shape[<span class="hljs-number">1</span>]<br><br>        x = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], h, w, p, p, c))<br>        x = torch.einsum(<span class="hljs-string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)<br>        imgs = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], c, h * p, h * p))<br>        <span class="hljs-keyword">return</span> imgs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, t, y</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Forward pass of DiT.</span><br><span class="hljs-string">        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)</span><br><span class="hljs-string">        t: (N,) tensor of diffusion timesteps</span><br><span class="hljs-string">        y: (N,) tensor of class labels</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x = self.x_embedder(x) + self.pos_embed  <span class="hljs-comment"># (N, T, D), where T = H * W / patch_size ** 2</span><br>        t = self.t_embedder(t)                   <span class="hljs-comment"># (N, D)</span><br>        y = self.y_embedder(y, self.training)    <span class="hljs-comment"># (N, D)</span><br>        c = t + y                                <span class="hljs-comment"># (N, D)</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            x = block(x, c)                      <span class="hljs-comment"># (N, T, D)</span><br>        x = self.final_layer(x, c)                <span class="hljs-comment"># (N, T, patch_size ** 2 * out_channels)</span><br>        x = self.unpatchify(x)                   <span class="hljs-comment"># (N, out_channels, H, W)</span><br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_with_cfg</span>(<span class="hljs-params">self, x, t, y, cfg_scale</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb</span><br>        half = x[: <span class="hljs-built_in">len</span>(x) // <span class="hljs-number">2</span>]<br>        combined = torch.cat([half, half], dim=<span class="hljs-number">0</span>)<br>        model_out = self.forward(combined, t, y)<br>        <span class="hljs-comment"># For exact reproducibility reasons, we apply classifier-free guidance on only</span><br>        <span class="hljs-comment"># three channels by default. The standard approach to cfg applies it to all channels.</span><br>        <span class="hljs-comment"># This can be done by uncommenting the following line and commenting-out the line following that.</span><br>        <span class="hljs-comment"># eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]</span><br>        eps, rest = model_out[:, :<span class="hljs-number">3</span>], model_out[:, <span class="hljs-number">3</span>:]<br>        cond_eps, uncond_eps = torch.split(eps, <span class="hljs-built_in">len</span>(eps) // <span class="hljs-number">2</span>, dim=<span class="hljs-number">0</span>)<br>        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)<br>        eps = torch.cat([half_eps, half_eps], dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> torch.cat([eps, rest], dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>
<p>如上图所示，x（images or latent representations of images）先经过PatchEmbed，与位置编码相加得到；y的embedding与t的embedding直接相加得到c，此后x与c经过若干个Dit Block。</p>
<figure>
<img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218214935006.png" srcset="/img/loading.gif" lazyload title="Dit Block图" alt="Dit Block图241218214935006" /><figcaption aria-hidden="true">Dit Block图241218214935006</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiTBlock</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads, mlp_ratio=<span class="hljs-number">4.0</span>, **block_kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=<span class="hljs-literal">True</span>, **block_kwargs)<br>        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        mlp_hidden_dim = <span class="hljs-built_in">int</span>(hidden_size * mlp_ratio)<br>        approx_gelu = <span class="hljs-keyword">lambda</span>: nn.GELU(approximate=<span class="hljs-string">&quot;tanh&quot;</span>)<br>        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=<span class="hljs-number">0</span>)<br>        self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">6</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(<span class="hljs-number">6</span>, dim=<span class="hljs-number">1</span>)<br>        x = x + gate_msa.unsqueeze(<span class="hljs-number">1</span>) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))<br>        x = x + gate_mlp.unsqueeze(<span class="hljs-number">1</span>) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<ol type="1">
<li>adaLN-Zero Block 是 <strong>adaptive LayerNorm-Zero</strong> Block 的简称。它是在传统的 Transformer 模块中引入了自适应的层归一化（Layer Normalization）机制，通过将条件信息（如扩散步数、标签等）动态地融入网络参数中，来提高生成模型的效果。它特别结合了 <strong>AdaLN</strong> 和 <strong>Zero Initialization</strong>，使得网络可以更好地适应不同的条件输入。</li>
</ol>
<p>在传统 Layer Normalization 的基础上，<strong>AdaLN</strong> 使用外部条件（例如时间步 <code>t</code>）来动态调整 LayerNorm 的尺度和偏移参数。具体表达为：</p>
<p><span class="math display">\[
AdaLN(h;\gamma(t),\beta(t))=\gamma(t) \cdot \frac{h-\mu}{\sigma} +\beta(t)
\]</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">modulate</span>(<span class="hljs-params">x, shift, scale</span>):<br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> + scale.unsqueeze(<span class="hljs-number">1</span>)) + shift.unsqueeze(<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = 		self.adaLN_modulation(c).chunk(<span class="hljs-number">6</span>, dim=<span class="hljs-number">1</span>)<br>        x = x + gate_msa.unsqueeze(<span class="hljs-number">1</span>) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))<br>        x = x + gate_mlp.unsqueeze(<span class="hljs-number">1</span>) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))<br>        <span class="hljs-keyword">return</span> x<br>  <br><br>self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">6</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br></code></pre></td></tr></table></figure>
<p>这里的<strong>6组可学习的参数</strong>由self.adaLN_modulation线性层，再通过chunk(6, dim=1)得到</p>
<p>这里的Multi-Head Self-Attention和Pointwise Feedforward分别为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=<span class="hljs-literal">True</span>, **block_kwargs)<br><br>self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<p>它称为 <strong>Pointwise</strong>，是因为它的操作不考虑不同位置之间的交互，仅在单个位置的向量上进行变换。</p>
<ul>
<li>Pointwise Feedforward Network 是 Transformer 架构中常见的模块，它对输入序列中的每个位置（token）的特征向量 <strong>独立</strong> 进行相同的前向传播运算（升维 -&gt; 激活 -&gt; 降维）</li>
<li>普通的 Feedforward Network 是标准的全连接神经网络，它将整个输入向量映射到输出向量，通过非线性激活函数引入非线性特性。</li>
</ul>
<ol start="2" type="1">
<li>Zero Initialization</li>
</ol>
<p><strong>Zero Initialization</strong> 的关键思想是在模块开始训练时，将残差块的输出初始化为零，以确保初始时模型的输入和输出保持一致。这种做法有两个好处：</p>
<ol type="1">
<li><strong>稳定训练</strong>：避免训练初期网络过早出现大幅度的变化。</li>
<li><strong>逐渐学习特征</strong>：残差分支在训练过程中逐渐学习有用的特征，从零开始优化。</li>
</ol>
<p>在实现上，网络的残差连接的输出会被一个动态缩放因子初始化为零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_weights</span>(<span class="hljs-params">self</span>):<br>    ...<br>  <br>		<span class="hljs-comment"># Zero-out adaLN modulation layers in DiT blocks:</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Zero-out output layers:</span><br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.bias, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>
<p><strong>Final Layer ——Transformer Decoder</strong></p>
<pre><code class="hljs">最后一个Dit Block后，需要将image tokens的序列解码为输出噪声以及对角的协方差矩阵的预测结果，且这两个输出的形状都与原始空间输入一致。作者在该环节使用标准的线性解码器，将每个token线性解码为p × p× 2C的张量，其中C是空间输入中到Dit的通道</code></pre>
<p>数，最后将解码的tokens重新排列到其原始空间布局中，得到预测的噪声和协方差。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FinalLayer</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The final layer of DiT.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size, patch_size, out_channels</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=<span class="hljs-literal">True</span>)<br>        self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">2</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift, scale = self.adaLN_modulation(c).chunk(<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        x = modulate(self.norm_final(x), shift, scale)<br>        x = self.linear(x)<br>        <span class="hljs-keyword">return</span> x   <span class="hljs-comment">##(N, T, patch_size ** 2 * out_channels)</span><br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">unpatchify</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    x: (N, T, patch_size**2 * C)</span><br><span class="hljs-string">    imgs: (N, H, W, C)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    c = self.out_channels<br>    p = self.x_embedder.patch_size[<span class="hljs-number">0</span>]<br>    h = w = <span class="hljs-built_in">int</span>(x.shape[<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>)<br>    <span class="hljs-keyword">assert</span> h * w == x.shape[<span class="hljs-number">1</span>]<br><br>    x = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], h, w, p, p, c))<br>    x = torch.einsum(<span class="hljs-string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)<br>    imgs = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], c, h * p, h * p))<br>    <span class="hljs-keyword">return</span> imgs<br></code></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self.out_channels = in_channels * <span class="hljs-number">2</span> <span class="hljs-keyword">if</span> learn_sigma <span class="hljs-keyword">else</span> in_channels<br></code></pre></td></tr></table></figure>
<p>说明了如果预测协方差矩阵的化，out_channels为2c，即第一部分c为噪声，第二部分c为预测的协方差矩阵</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Diffusion Transformer详解</div>
      <div>https://jiachengc320.github.io/2024/12/15/Diffusion_Transformer详解/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Jiacheng Chen</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>December 15, 2024</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/12/18/stable-diffusion%E8%AF%A6%E8%A7%A3/" title="stable diffusion详解">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">stable diffusion详解</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/12/03/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83/" title="大模型无监督训练">
                        <span class="hidden-mobile">大模型无监督训练</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.utils.listenDOMLoaded(function() {
      Fluid.events.registerRefreshCallback(function() {
        if ('mermaid' in window) {
          mermaid.init();
        }
      });
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
