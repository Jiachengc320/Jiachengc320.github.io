<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>扩散模型解读系列</title>
    <link href="/2024/12/15/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97/"/>
    <url>/2024/12/15/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E8%A7%A3%E8%AF%BB%E7%B3%BB%E5%88%97/</url>
    
    <content type="html"><![CDATA[<h1 id="vit架构及介绍">Vit架构及介绍</h1><p>vit是谷歌提出的把Transformer应用到图像分类的模型，论文的最核心结论是：<strong>当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果</strong></p><p>当训练数据集不够大的时候，Vit通常比同等大小的Resnets要差一些，因为Transformer和CNN相比缺少归纳偏置，即一种先验知识。（CNN具有两种归纳偏置，1.局部性，图片上相邻区域具有相似的特征；2.平移不变性。CNN具有上述两种归纳偏置，具有先验信息，相对少的数据就可以学到一个比较好的模型）</p><h2 id="vit的架构">Vit的架构</h2><p>Vit将输入图片分为多个patch(16x16)，再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同，但需要对图像进行分类，故而在输入序列中加入一个特殊的token。该token对应的输出即为最后的类别预测</p><h2 id="步骤">步骤</h2><ul><li><p>patch embedding：如输入的图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，每张图片会生成196个patch，每个patch的维度为16x16x3，输入通过线性投影层后维度依然为196x768。<strong>这里需要加一个特殊的字符cls</strong>，则最终维度为<strong>197 x 768</strong> 。将视觉问题转化为了一个seq2seq问题。</p></li><li><p>positional encoding ：Vit需要加入位置编码。位置编码可理解成一张表，共N行，与输入序列长度相同(197)，每行代表一个向量，向量维度与输入序列embedding的维度相同（768）。编码的操作是sum</p></li><li><p>LN/multi-head attention/LN 维度依然是197x768</p></li><li><p>MLP：将维度放大，再缩回去，197x768-&gt;197x3072-&gt;197x768。</p><p>一个Block之后维度保持不变，因此可以堆叠多个，最后将特殊字符cls对应的输出<span class="math inline">\(z_L^0\)</span>作为encoder的最终输出。</p><p>（另一种做法：不加cls字符，对所有tokens的输出做一个平均）</p></li></ul><h2 id="关于positional-encoding">关于positional encoding</h2><ul><li>1-D 位置编码， 1-9</li><li>2-D 位置编码， 11,12，... ,33</li></ul><p><strong>奇怪现象：1-D，2-D，不适用位置编码，性能非常接近，即使不使用位置编码，模型的性能损失也不是很大。</strong></p><p>解释：</p><ol type="1"><li><p>Patch-Level Processing(最主要原因)</p><p>VIT模型将图像分为多个小块，并将每个小块作为一个单独的输入。相比于处理像素级别的信息，这些小块之间的相对位置更容易被模型理解和利用。通过这些小块进行自注意力机制，能够捕捉到他们之间的空间关系</p></li><li><p>transformer的自注意力机制：</p></li></ol><p>Vit的Transformer 层具有强大的自注意力机制，可以在一个序列中的不同位置之间建立复杂的关联</p><ol start="3" type="1"><li>模型的学习能力。</li></ol><p>优点：</p><ol type="1"><li>全局感知特性：ViT能够在全局范围内捕捉特征和上下文关系，源于它的自注意力机制，能够理解图像的全局上下文</li><li>数据效率：ViT在大规模数据集上表现优秀，这使得ViT可以更有效利用那些包含超大量样本的数据集</li><li>数据通用性：ViT借鉴了transformer模型，具有很强的迁移学习能力，更有利于后续的模态融合。</li></ol><p>缺点：</p><ol type="1"><li>需要大量数据：ViT要活的良好性能，需要大量训练数据，对于小规模数据集不如CNN</li><li>计算成本高：ViT的计算和内存需求比传统的CNN大，特别在高分辨率图像中，自注意力的计算复杂度会随着像素数量的增加而增加</li><li>可解释性差：尽管Transformer结构在很多任务上有很好的表现，但它的可解释性往往较差。这是因为ViT模型学到的全局信息和关系，可能并不直观或容易理解，这给模型的理解和优化带来了困难。</li></ol><p><strong>ViT适合在哪些场景下应用：</strong></p><ol type="1"><li><strong>大规模图像识别</strong>：ViT在数据丰富的环境下表现尤其突出，因此适合需要在大规模数据集上进行复杂图像识别任务的场景，例如图像分类、物体识别等。</li><li><strong>高清图像处理</strong>：利用它的全局感知和高分辺率处理能力，ViT适用于高清图像的分析，如医学成像、卫星图像分析等。</li><li><strong>迁移学习任务</strong>：ViT的模型通用性使它在迁移学习场景中表现良好，可以应用于那些与预训练数据领域相近的任务上。</li><li><strong>多模态学习</strong>：因为Transformer架构的灵活性，ViT可以与其他模式的Transformer（如用于文本或音频的Transformer）结合，进行多模态学习，比如图像-文本匹配等。</li><li><strong>科学研究</strong>：在需要精确理解图像全局上下文的科学研究中，比如地理信息系统(GIS)、天体物理学等，ViT由于其全局感知特性也非常有用。</li></ol><h1 id="dit-架构及介绍">Dit 架构及介绍</h1><p>《Scalable Diffusion Models with Transformers》</p><h2 id="patchify过程">Patchify过程</h2><ul><li><p>图像的大小为<span class="math inline">\(I \times I \times c\)</span> ，将图像Patchify，并经过Linear Embedding，变成T个d维的tokens，经过Linear Embedding，得到T个d维的tokens。</p></li><li><p>Patchify后，作者将标准的基于Vit频率的位置编码（sin-cosine）应用到输入的tokens上</p><p>作者在Dit design space里使用p=2,4,8，token T的数量由Patch大小p决定，满足<span class="math inline">\(T=(I/p)^2\)</span></p></li></ul><h2 id="dit-block设计">Dit Block设计</h2><p>Patchify后，输入的tokens开始进入一系列的Transformer Block中，Diffusion Model会额外处理条件信息，如噪声时间步长t，标签c,自然语言text。</p><p>作者探索了4种不同类型的Transformer Block，以不同的方式处理条件输入，这些设计对标准Vit Block进行微小修改。</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-5efe86e40ab87a05d33a2e8c75539f72_r.jpg" title="Dit 框架图" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li>In-Context Conditioning</li></ul><p>带条件输入的情况，该方法将时间步长t，类标签c作为两个额外的token附加到输入的序列中。</p><p>作者将其视为与图像的token无区别，允许Dit使用标准的Vit Block（无修改），经过最后一个Block后，删除条件token即可。</p><p>In-Context Conditioning带来的额外 GFLOPs 微不足道。</p><ul><li>Cross-Attention Block</li></ul><p>该方法将时间步长t和类标签c的Embedding连接成一个长度为2的Sequence，且与image token序列分开，该方法给Transformer Block添加一个Cross-Attention块。</p><p>Cross-Attention Block带来的额外GFLOPs开销大约15%。</p><ul><li>Adaptive Layer Norm（adaLN）Block</li></ul><p>adaLN Block遵循GAN中的自适应归一化层，探索这个在扩散模型里是否适用。本文作者并未学习缩放和移位参数<span class="math inline">\(\gamma\)</span> 和<span class="math inline">\(\beta\)</span>，而是改用噪声时长t和类标签c得到。</p><p>Adaptive Layer Norm（adaLN）Block带来的额外的 GFLOPs 是最少的。</p><ul><li>adaLN-Zero Block</li></ul><p>有监督学习中，对于每个Block的第一个Batch Norm操作的缩放因子进行Zero-Initialization可以加速其大规模训练。基于U-Net的扩散模型使用类似的初始化策略，对每个Block的第一个卷积进行Zero-Initialization。本文作者做了一些改进：回归计算缩放参数<span class="math inline">\(\gamma\)</span>和移位参数<span class="math inline">\(\beta\)</span>外，还回归缩放系数<span class="math inline">\(\alpha\)</span>。</p><p>作者初始化MLP使其输出的缩放系数<span class="math inline">\(\alpha\)</span> 全部为0，使得Dit Block初始化为恒等函数（Identity Function）。</p><p>adaLN-Zero Block带来的额外的GFLOPs可忽略不计</p><h2 id="transformer-decoder">Transformer Decoder</h2><p>在最后一个Dit Block后，需要将image tokens的序列解码为输出噪声以及对角的协方差矩阵的预测结果，且这两个输出的形状都与原始空间输入一致。作者在该环节使用标准的线性解码器，将每个token线性解码为<span class="math inline">\(p \times p \times 2C\)</span>的张量，其中<span class="math inline">\(C\)</span>是空间输入中到Dit的通道数，最后将解码的tokens重新排列到其原始空间布局中，得到预测的噪声和协方差。</p><h2 id="dit-训练策略">Dit 训练策略</h2><h3 id="训练设置">训练设置</h3><p>作者在ImageNet 数据集上训练class-conditional latent Dit模型，分辨率设为256x256和512x512，对最后的Linear Layer使用Zero Initialization,其余使用Vit的标准权重初始化方案。优化器使用AdamW</p><h3 id="扩散模型配置">扩散模型配置</h3><p>Dit其他组件，作者中Stable Diffusion预训练好的Variational AutoEncoder模型，下采样率是8，给定256x246x3的输入图片x，得到的编码结果z尺寸为32x32x4，从扩散模型中Sample出一个新的latent结果后，作者使用VAE decoder，将其解码回pixel:<span class="math inline">\(x=D(z)\)</span></p><h2 id="代码解析">代码解析</h2><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218214457127.png" title="Dit 模块图" alt="image-20241218214457127" /><figcaption aria-hidden="true">image-20241218214457127</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiT</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Diffusion model with a Transformer backbone.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_size=<span class="hljs-number">32</span>,</span><br><span class="hljs-params">        patch_size=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">        in_channels=<span class="hljs-number">4</span>,</span><br><span class="hljs-params">        hidden_size=<span class="hljs-number">1152</span>,</span><br><span class="hljs-params">        depth=<span class="hljs-number">28</span>,</span><br><span class="hljs-params">        num_heads=<span class="hljs-number">16</span>,</span><br><span class="hljs-params">        mlp_ratio=<span class="hljs-number">4.0</span>,</span><br><span class="hljs-params">        class_dropout_prob=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">        num_classes=<span class="hljs-number">1000</span>,</span><br><span class="hljs-params">        learn_sigma=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.learn_sigma = learn_sigma<br>        self.in_channels = in_channels<br>        self.out_channels = in_channels * <span class="hljs-number">2</span> <span class="hljs-keyword">if</span> learn_sigma <span class="hljs-keyword">else</span> in_channels<br>        self.patch_size = patch_size<br>        self.num_heads = num_heads<br><br>        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=<span class="hljs-literal">True</span>)<br>        self.t_embedder = TimestepEmbedder(hidden_size)<br>        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)<br>        num_patches = self.x_embedder.num_patches<br>        <span class="hljs-comment"># Will use fixed sin-cos embedding:</span><br>        self.pos_embed = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>, num_patches, hidden_size), requires_grad=<span class="hljs-literal">False</span>)<br><br>        self.blocks = nn.ModuleList([<br>            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(depth)<br>        ])<br>        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)<br>        self.initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># Initialize transformer layers:</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_basic_init</span>(<span class="hljs-params">module</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>                torch.nn.init.xavier_uniform_(module.weight)<br>                <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    nn.init.constant_(module.bias, <span class="hljs-number">0</span>)<br>        self.apply(_basic_init)<br><br>        <span class="hljs-comment"># Initialize (and freeze) pos_embed by sin-cos embedding:</span><br>        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-<span class="hljs-number">1</span>], <span class="hljs-built_in">int</span>(self.x_embedder.num_patches ** <span class="hljs-number">0.5</span>))<br>        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).<span class="hljs-built_in">float</span>().unsqueeze(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># Initialize patch_embed like nn.Linear (instead of nn.Conv2d):</span><br>        w = self.x_embedder.proj.weight.data<br>        nn.init.xavier_uniform_(w.view([w.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>]))<br>        nn.init.constant_(self.x_embedder.proj.bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Initialize label embedding table:</span><br>        nn.init.normal_(self.y_embedder.embedding_table.weight, std=<span class="hljs-number">0.02</span>)<br><br>        <span class="hljs-comment"># Initialize timestep embedding MLP:</span><br>        nn.init.normal_(self.t_embedder.mlp[<span class="hljs-number">0</span>].weight, std=<span class="hljs-number">0.02</span>)<br>        nn.init.normal_(self.t_embedder.mlp[<span class="hljs-number">2</span>].weight, std=<span class="hljs-number">0.02</span>)<br><br>        <span class="hljs-comment"># Zero-out adaLN modulation layers in DiT blocks:</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Zero-out output layers:</span><br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.bias, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unpatchify</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        x: (N, T, patch_size**2 * C)</span><br><span class="hljs-string">        imgs: (N, H, W, C)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        c = self.out_channels<br>        p = self.x_embedder.patch_size[<span class="hljs-number">0</span>]<br>        h = w = <span class="hljs-built_in">int</span>(x.shape[<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>)<br>        <span class="hljs-keyword">assert</span> h * w == x.shape[<span class="hljs-number">1</span>]<br><br>        x = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], h, w, p, p, c))<br>        x = torch.einsum(<span class="hljs-string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)<br>        imgs = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], c, h * p, h * p))<br>        <span class="hljs-keyword">return</span> imgs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, t, y</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Forward pass of DiT.</span><br><span class="hljs-string">        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)</span><br><span class="hljs-string">        t: (N,) tensor of diffusion timesteps</span><br><span class="hljs-string">        y: (N,) tensor of class labels</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x = self.x_embedder(x) + self.pos_embed  <span class="hljs-comment"># (N, T, D), where T = H * W / patch_size ** 2</span><br>        t = self.t_embedder(t)                   <span class="hljs-comment"># (N, D)</span><br>        y = self.y_embedder(y, self.training)    <span class="hljs-comment"># (N, D)</span><br>        c = t + y                                <span class="hljs-comment"># (N, D)</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            x = block(x, c)                      <span class="hljs-comment"># (N, T, D)</span><br>        x = self.final_layer(x, c)                <span class="hljs-comment"># (N, T, patch_size ** 2 * out_channels)</span><br>        x = self.unpatchify(x)                   <span class="hljs-comment"># (N, out_channels, H, W)</span><br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_with_cfg</span>(<span class="hljs-params">self, x, t, y, cfg_scale</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb</span><br>        half = x[: <span class="hljs-built_in">len</span>(x) // <span class="hljs-number">2</span>]<br>        combined = torch.cat([half, half], dim=<span class="hljs-number">0</span>)<br>        model_out = self.forward(combined, t, y)<br>        <span class="hljs-comment"># For exact reproducibility reasons, we apply classifier-free guidance on only</span><br>        <span class="hljs-comment"># three channels by default. The standard approach to cfg applies it to all channels.</span><br>        <span class="hljs-comment"># This can be done by uncommenting the following line and commenting-out the line following that.</span><br>        <span class="hljs-comment"># eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]</span><br>        eps, rest = model_out[:, :<span class="hljs-number">3</span>], model_out[:, <span class="hljs-number">3</span>:]<br>        cond_eps, uncond_eps = torch.split(eps, <span class="hljs-built_in">len</span>(eps) // <span class="hljs-number">2</span>, dim=<span class="hljs-number">0</span>)<br>        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)<br>        eps = torch.cat([half_eps, half_eps], dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> torch.cat([eps, rest], dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>如上图所示，x（images or latent representations of images）先经过PatchEmbed，与位置编码相加得到；y的embedding与t的embedding直接相加得到c，此后x与c经过若干个Dit Block。</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218214935006.png" title="Dit Block图" alt="image-20241218214935006" /><figcaption aria-hidden="true">image-20241218214935006</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiTBlock</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads, mlp_ratio=<span class="hljs-number">4.0</span>, **block_kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=<span class="hljs-literal">True</span>, **block_kwargs)<br>        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        mlp_hidden_dim = <span class="hljs-built_in">int</span>(hidden_size * mlp_ratio)<br>        approx_gelu = <span class="hljs-keyword">lambda</span>: nn.GELU(approximate=<span class="hljs-string">&quot;tanh&quot;</span>)<br>        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=<span class="hljs-number">0</span>)<br>        self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">6</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(<span class="hljs-number">6</span>, dim=<span class="hljs-number">1</span>)<br>        x = x + gate_msa.unsqueeze(<span class="hljs-number">1</span>) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))<br>        x = x + gate_mlp.unsqueeze(<span class="hljs-number">1</span>) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><ol type="1"><li>adaLN-Zero Block 是 <strong>adaptive LayerNorm-Zero</strong> Block 的简称。它是在传统的 Transformer 模块中引入了自适应的层归一化（Layer Normalization）机制，通过将条件信息（如扩散步数、标签等）动态地融入网络参数中，来提高生成模型的效果。它特别结合了 <strong>AdaLN</strong> 和 <strong>Zero Initialization</strong>，使得网络可以更好地适应不同的条件输入。</li></ol><p>在传统 Layer Normalization 的基础上，<strong>AdaLN</strong> 使用外部条件（例如时间步 <code>t</code>）来动态调整 LayerNorm 的尺度和偏移参数。具体表达为：</p><p><span class="math display">\[AdaLN(h;\gamma(t),\beta(t))=\gamma(t) \cdot \frac{h-\mu}{\sigma} +\beta(t)\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">modulate</span>(<span class="hljs-params">x, shift, scale</span>):<br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> + scale.unsqueeze(<span class="hljs-number">1</span>)) + shift.unsqueeze(<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(<span class="hljs-number">6</span>, dim=<span class="hljs-number">1</span>)<br>        x = x + gate_msa.unsqueeze(<span class="hljs-number">1</span>) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))<br>        x = x + gate_mlp.unsqueeze(<span class="hljs-number">1</span>) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))<br>        <span class="hljs-keyword">return</span> x<br>  <br><br>self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">6</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br></code></pre></td></tr></table></figure><p>这里的<strong>6组可学习的参数</strong>由self.adaLN_modulation线性层，再通过chunk(6, dim=1)得到</p><p>这里的Multi-Head Self-Attention和Pointwise Feedforward分别为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=<span class="hljs-literal">True</span>, **block_kwargs)<br><br>self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>它称为 <strong>Pointwise</strong>，是因为它的操作不考虑不同位置之间的交互，仅在单个位置的向量上进行变换。</p><ul><li>Pointwise Feedforward Network 是 Transformer 架构中常见的模块，它对输入序列中的每个位置（token）的特征向量 <strong>独立</strong> 进行相同的前向传播运算（升维 -&gt; 激活 -&gt; 降维）</li><li>普通的 Feedforward Network 是标准的全连接神经网络，它将整个输入向量映射到输出向量，通过非线性激活函数引入非线性特性。</li></ul><ol start="2" type="1"><li>Zero Initialization</li></ol><p><strong>Zero Initialization</strong> 的关键思想是在模块开始训练时，将残差块的输出初始化为零，以确保初始时模型的输入和输出保持一致。这种做法有两个好处：</p><ol type="1"><li><strong>稳定训练</strong>：避免训练初期网络过早出现大幅度的变化。</li><li><strong>逐渐学习特征</strong>：残差分支在训练过程中逐渐学习有用的特征，从零开始优化。</li></ol><p>在实现上，网络的残差连接的输出会被一个动态缩放因子初始化为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_weights</span>(<span class="hljs-params">self</span>):<br>    ...<br>  <br><span class="hljs-comment"># Zero-out adaLN modulation layers in DiT blocks:</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Zero-out output layers:</span><br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.bias, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p><strong>Final Layer ——Transformer Decoder</strong></p><pre><code class="hljs">最后一个Dit Block后，需要将image tokens的序列解码为输出噪声以及对角的协方差矩阵的预测结果，且这两个输出的形状都与原始空间输入一致。作者在该环节使用标准的线性解码器，将每个token线性解码为p × p× 2C的张量，其中C是空间输入中到Dit的通道</code></pre><p>数，最后将解码的tokens重新排列到其原始空间布局中，得到预测的噪声和协方差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FinalLayer</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The final layer of DiT.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size, patch_size, out_channels</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=<span class="hljs-literal">True</span>)<br>        self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">2</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift, scale = self.adaLN_modulation(c).chunk(<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        x = modulate(self.norm_final(x), shift, scale)<br>        x = self.linear(x)<br>        <span class="hljs-keyword">return</span> x   <span class="hljs-comment">##(N, T, patch_size ** 2 * out_channels)</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">unpatchify</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    x: (N, T, patch_size**2 * C)</span><br><span class="hljs-string">    imgs: (N, H, W, C)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    c = self.out_channels<br>    p = self.x_embedder.patch_size[<span class="hljs-number">0</span>]<br>    h = w = <span class="hljs-built_in">int</span>(x.shape[<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>)<br>    <span class="hljs-keyword">assert</span> h * w == x.shape[<span class="hljs-number">1</span>]<br><br>    x = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], h, w, p, p, c))<br>    x = torch.einsum(<span class="hljs-string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)<br>    imgs = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], c, h * p, h * p))<br>    <span class="hljs-keyword">return</span> imgs<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self.out_channels = in_channels * <span class="hljs-number">2</span> <span class="hljs-keyword">if</span> learn_sigma <span class="hljs-keyword">else</span> in_channels<br></code></pre></td></tr></table></figure><p>说明了如果预测协方差矩阵的化，out_channels为2c，即第一部分c为噪声，第二部分c为预测的协方差矩阵</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型无监督训练</title>
    <link href="/2024/12/03/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83/"/>
    <url>/2024/12/03/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<p>无监督训练是机器学习的一种方式，它不依赖于带标签的数据来训练模型，而是通过从未标注的数据中发现潜在的结构或模式。对于语言模型，无监督学习通常意味着让模型通过大量的文本数据自我学习，从而能够理解语言的结构、上下文、语法规则。</p><h2 id="无监督训练方法">无监督训练方法</h2><ul><li>自编码器：通过将输入映射到潜在空间，再从潜在空间恢复回输入来训练，学习数据的低维表示</li><li>聚类：聚类算法视图将数据集分城若干类别或簇，模型通过这些簇来发现数据中的结构</li><li>生成模型：生成式模型视图学习数据的分布，并从中生成新的数据</li></ul><h2 id="无监督训练在llm中的应用">无监督训练在LLM中的应用</h2><p>LLM通常采用的无监督训练方式是通过<strong>自回归训练</strong>（autoregressive training）或者<strong>掩码语言建模</strong>（masked language modeling）来训练。</p><ul><li>自回归训练语言建模：模型的目标是预测一个句子中的每个单词的下一个单词，对于大量未标注的文本数据，模型通过这种方式逐步学习语言的结构和上下文信息</li><li>掩码语言建模：通常用于Bert类模型，句子随机掩盖掉一些词，模型的任务是根据上下文信息预测被掩盖的词。例如，在句子“The cat sat on the [MASK]”中，模型需要根据上下文预测出被掩盖的词“mat”</li></ul><p>2）从未标注的文本中学习表示</p><p>无监督训练的另一个关键应用是让模型通过海量的文本数据学习到词语的表示，如词嵌入是通过无监督方法生成的，它能够将单词映射到高维空间，使得语义相近的词在该空间中更为接近。</p><p>3）迁移学习</p><p>无监督训练往往是LLM的预训练阶段，而在实际应用时，模型可能会根据特定任务进行微调。</p><p>无监督训练在大规模语言模型（LLM）中的应用是至关重要的，它帮助模型从大量未标注的文本中学习语言的结构、上下文以及潜在的语义关系。通过无监督学习，LLM能够掌握丰富的语言知识，并为下游任务提供强大的支持。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型微调方式</title>
    <link href="/2024/12/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F/"/>
    <url>/2024/12/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="全量微调-full-fine-tuning">全量微调 Full Fine-tuning</h1><p>原理：在预训练的大型模型基础上，对模型的所有层和参数进行调整，使其适应特定任务。在这个过程中，模型会根据特定任务的数据重新学习和更新所有的权重参数，以达到更好地完成该任务的目的。</p><p>优点：可以充分利用预训练模型的通用特征，较好地适应特定任务。</p><p>缺点：计算成本高</p><h1 id="基于适配器adapter的微调">基于适配器（Adapter）的微调：</h1><p>原理：在预训练模型的每一层或某些层中添加适配器模块，微调时冻结预训练模型主体。由适配器模块学习特定下游任务的知识。</p><p>适配器由两个前馈子层组成，第一个将模型的输出作为输入，投影到较小维度，第二个将其还原到原始输入维度作为输出</p><p>优点：只需要训练少量的特定于任务的参数，降低了训练的计算成本和存储需求，预训练模型的主体被冻结，保留预训练模型的大部分知识，能快速适应新的任务。</p><p>缺点：增加了模型的复杂性，可能影响模型的推理速度，且适配器模块的设计要求较高</p><h1 id="基于低秩适应lora的微调">基于低秩适应（LORA）的微调</h1><p>Low-Rank Adaptation of Large Language Models</p><p>原理：冻结预训练模型的矩阵参数，并引入额外的低秩矩阵代替模型权重的变化</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241122143906738.png" alt="image-20241122143906738" /><figcaption aria-hidden="true">image-20241122143906738</figcaption></figure><p><span class="math display">\[h=W_0 x+\Delta Wx=W_0 x+BAx\]</span></p><p>学习的是BA矩阵，来量化权重的差异<span class="math inline">\(\Delta W\)</span></p><h1 id="p-tuning">P-Tuning</h1><p>为了解决人工设计的Prompt问题： GPT Understands,TOO</p><ul><li>大模型的Prompt构造方式严重影响下游任务的效果，如GPT3采用人工构造的模板做上下文学习，模板的变化较敏感，加减词或变动位置会造成较大变化</li><li>自动化搜索模板工作成本较高，离散化token搜索出的结果可能并不最优，性能不稳定</li></ul><p>P-Tuning将提示Prompt转化为可学习的Embedding层，并使用MLP+LSTM的方式对Prompt Embedding进行一层处理。</p><ul><li>P-Tuning 设计了一种连续可微的virtual token，将Prompt转换为可学习的Embedding层，使用MLP+LSTM的方式对Prompt Embedding进行一层处理。</li><li>经过预训练的LM的词嵌入变得高度离散，若随机初始化virtual token，容易优化到局部最优，作者通过实验发现用一个prompt encoder编码收敛更快，效果更好。"virtual token"指的是一种用于微调语言模型的特殊标记。</li></ul><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241122155804651.png" alt="image-20241122155804651" /><figcaption aria-hidden="true">image-20241122155804651</figcaption></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/10/10/hello-world/"/>
    <url>/2023/10/10/hello-world/</url>
    
    <content type="html"><![CDATA[<pre class="mermaid">    graph TD;      A-->B;      A-->C;      B-->D;      C-->D;</pre><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
