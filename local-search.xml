<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>多模态笔记</title>
    <link href="/2024/12/24/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%AC%94%E8%AE%B0/"/>
    <url>/2024/12/24/%E5%A4%9A%E6%A8%A1%E6%80%81%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="bert">Bert</h1><p>Google 论文Pre-training of Deep Bidirectional Transformers for Language Understanding</p><p>整体是一个自编码语言模型（Autoencoder LM），设计了两个任务来预训练该模型</p><ul><li>采用MaskLM的方式训练语言模型：输入一句话的时候，随机地选一些要预测的词，然后用一个特殊的符号[MASK]来代替它们，之后让模型根据所给的标签去学习这些地方该填的词。</li><li>在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入BERT的两端文本是否为连续的文本，引入这个任务可以更好地让模型学到连续的文本片段之间的关系。</li></ul><p>BERT 相较于原来的 RNN、LSTM 可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义。相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。同时缺点也是显而易见的，模型参数太多，而且模型太大，少量数据训练时，容易过拟合。</p><p>BERT 只使用了 Transformer 的 Encoder 模块，原论文中，作者分别用 12 层和 24 层 Transformer Encoder 组装了两套 BERT 模型，分别是:</p><ul><li><span class="math inline">\(BERT_{BASE}:L=12,H=768,A=12,TotalParameters=10M\)</span></li><li><span class="math inline">\(BERT_{LARGE}:L=24,H=1024,A=16,TotalParameters=340M\)</span></li></ul><p>L：Transformer Encoder块的数量，H：隐藏层维度，A：自注意力个头数，下例中Transformer Encoder端 [^Feedforward Layer] 层的维度设置为4H</p><p><strong>与 Transformer 本身的 Encoder 端相比，BERT 的 Transformer Encoder 端输入的向量表示，多了 Segment Embeddings</strong></p><p>[^Feedforward Layer]: （前馈层）是一种最基本的网络层，它在处理数据时只向前传递（从输入到输出）。这种层通常用于构建更复杂的网络结构，如卷积神经网络（CNN）或循环神经网络（RNN）中。</p><blockquote><p>https://blog.csdn.net/weixin_44799217/article/details/115374101</p></blockquote><h1 id="blip-模型-2022">Blip 模型 2022</h1><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20240816114457355.png" alt="image-20240816114457355" /><figcaption aria-hidden="true">image-20240816114457355</figcaption></figure><p>NLP,CV 通过transformer无缝兼容</p><p>Attention: QKV</p><p>最终目的：文生图</p><p><strong>Blip2.0 2023.3 文生图</strong></p><h1 id="clip模型">Clip模型</h1><p>经典分类模型（如resnet）的问题：</p><ol type="1"><li>类别固定</li><li>当前的模型只能胜任一个任务，迁移到新任务上非常困难</li><li>类别互斥</li><li>当前的cv数据集标注劳动密集，成本高昂。</li><li>当前模型泛化能力较差</li></ol><p>双塔结构</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20240816115919428.png" alt="image-20240816115919428" /><figcaption aria-hidden="true">image-20240816115919428</figcaption></figure><p>Text Encoder: bert/gpt等</p><p>Image Encoder: resnet/vgg/vit</p><p>正负样本在batchsize里内部构造出来的，Batchsize是随机得到，训练样本也具有随机性</p><p>Batchsize=N, Batchsize选多大合适？</p><ol type="1"><li>如果batchsize过小，负样本太少，训练效果不佳</li><li>batchsize不能太大，在类别数较小的时候，负样本不准</li></ol><p>模型主要是训练Text Encoder 和Image Encoder，其将文本信息和图像信息投射在同一个空间里</p><p>0样本分类，离线计算</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20240816133429600.png" alt="image-20240816133429600" /><figcaption aria-hidden="true">image-20240816133429600</figcaption></figure><p>对于Text-Encoder,作者统一采用GPT-2里的Transformer结构，对于base size model，使用63M-parameter 12-layer 512-width model with 8 attention heads; model width随着image encoder 的size增加而增加。输入句子的最大长度为76</p><p>Clip能够zero shot识别，而且效果不错的原因在于：</p><ol type="1"><li>训练集够大，zero shot任务的图像分布在训练集中有类似的，zero shot任务的concept在训练集中有相近的</li><li>将分类问题转换为检索问题</li></ol><p>训练数据通过50W条query,搜索得到4亿个图像文本对</p><h2 id="limitation">Limitation</h2><ol type="1"><li>CLIP的zero-shot性能虽然总体上比supervised baseline ResNet-50好，但其实很多任务上比不过SOTA methods，因此CLIP的transfer learning有待挖掘</li><li>CLIP在几种task上zero-shot性能不好：fine-grained分类（花的分类，车的分类之类的）、抽象的任务（如计算图中object的个数）以及预训练时没见过的task（如分出相邻车辆的距离）</li><li>Zero-shot CLIP在真正意义上的out-of-distribution data上性能不好，比如OCR中</li><li>生成新的概念（如：词），这是CLIP功能上的缺陷，CLIP终究不是生成模型</li><li>CLIP的训练数据是从网上采集的，这些image-text pairs没有做data-clear 和de-bias，这可能会使得模型有一些social biases</li><li>很多视觉任务很难用text来表达，如何用更高效的few-shot learning方法优化CLIP也很重要（通过大模型语言LLAMA可能可以解决）</li></ol><h1 id="sam模型">SAM模型</h1><p>目标分割：像素级的对前景与背景进行分类，将背景剔除</p><p><a href="segment-anything.com">Demo</a></p><p>根据demo可以发现，我们可以发现，通过在图像中指定要分割的内容提示，SAM可以实现各种分割任务，且无需额外的训练、做到零样本泛化，即SAM学会了辨别物体、具备图像理解力、对不熟悉的图像和物体能进行零样本概括，这一通用特性使得SAM模型在有关领域的推广应用成为可能。</p><p>牛逼之处：0样本学习 prompt</p><p>SAM模型设计的出发点</p><blockquote><p>零样本学习：</p><ol type="1"><li><p>什么任务可以实现零样本泛化？</p><p>文章认为，手续爱你需要定义一个可提示的分割任务，该任务足够通用，以提供强大的预训练目标并支持广泛的下游应用程序</p></li><li><p>对应的模型架构是怎样的？</p><p>文章指出，需要一个支持灵活提示的模型，并且可以在提示时实时输出分割掩码，以供交互使用</p></li><li><p>哪些数据可以为这项任务和模型提供支持？</p><p>文章提出，训练模型需要多样化、大规模的数据源，为解决这一问题，可以构建一个"数据引擎"，即在使用高效模型来协助数据收集和使用新收集的数据来改进模型之间进行迭代</p></li></ol></blockquote><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20240816165952779.png" alt="image-20240816165952779" /><figcaption aria-hidden="true">image-20240816165952779</figcaption></figure><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20240816170306638.png" alt="image-20240816170306638" /><figcaption aria-hidden="true">image-20240816170306638</figcaption></figure><p>prompt encoder：prompt总共有point,box,mask,text四种，会将其分为三类</p><ul><li>point和box可以作为一类使用position encodings</li><li>text可以使用CLIP作为encoder</li><li>mask是一种密集型prompt，可以使用卷积作为encoder</li></ul><p>类别极度不均衡场景：</p><p>focal loss <span class="math display">\[L_{fl} = \begin{cases} -(1 - \hat{p})^{\gamma} \log(\hat{p}) &amp; \text{if } y = 1 \\-\hat{p}^{\gamma} \log(1 - \hat{p}) &amp; \text{if } y = 0 \end{cases}\]</span> <span class="math inline">\(\gamma =0\)</span> 传统分类，<span class="math inline">\(\gamma\)</span> 越大,越难分。</p><p>通过系数来控制好分和难分的比重</p><h1 id="glip模型-2023.3-目标检测">GLIP模型 2023.3 目标检测</h1><p>Grounded Language-Image Pre-training(GLIP)</p><p><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241224163955952.png" /></p><p>从模型输入层面来说，GLIP 创造性地将目标检测任务转换为短语定位任务</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>sora详解</title>
    <link href="/2024/12/20/sora%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/20/sora%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="sora">Sora</h1><p>Key Message 1：Sora的整体结构如下</p><p><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241220171250175.png" /></p><ul><li>多分辨率支持 可提升视频生成质量</li><li>图片 &amp;&amp; 视频联合训练</li><li>长时一致性（60s视频）</li></ul><p>图片与视频的联合训练:带标注图像数据比带视频标注数据多得多（LAION 5B），这是Sora效果好的重要原因之一</p><p>model trained with 80% video and 20% image data</p><p>key Message 2 :</p><p>Sora的Visual Encoder-Decoder很可能采用了TECO（Temporally Consistent Transformer）模型的思路。Encodaer-Decoder部分的核心可能在于：为生成长达60s的高质量视频，如何维护长时一致性最为关键。需要在信息压缩与输入部分急躁融入视频的长时一致性信息，不能只靠Diffusion Model</p><p>Key Message 3: Sora把Patch部分称为 Spacetime Latent Patch，大概采用了NaVIT的思路，而不是Padding方案</p><h2 id="teco-模型">TECO 模型</h2><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241224131753550.png" alt="置信度99%" /><figcaption aria-hidden="true">置信度99%</figcaption></figure><p>用户给出相对较短且描述粗略的Prompt后，Sora先用GPT对用户Prompt进行扩写，扩充出包含细节描述的长Prompt，Sora技术报告明确提到的。（Prompt对细节描述得越明确，视频生成质量越好），体现在尽可能多的生产环节让大模型辅助或取代人的思路。</p><p>Text Encoder大概率是CLIP模型对应的文本编码器</p><p>Sora应该按照视频逐帧生成的方式，如用户希望生成10s长度，分辨率为1080x1080的视频，按照电影级标准"24帧/秒"流畅度，Sora需要生成240帧1080x1080的图片。可以在生成视频前，事先昌盛好240帧1080x1080的噪声图，然后Sora在用户Prompt语义的指导下，按照时间顺序，生成符合Prompt描述的240张视频帧对应图片，形成视频生成结果。</p><p>Latent Diffusion Model节省资源速度快，效果比PDM差，因此Sora大概率采用LDM，且在Encoder这一步不会压缩得太狠，保留更多原始图片细节信息，以资源换质量，Sora生成视频速度慢可能和Encoder压缩率不高有一定关系</p><p>对视频帧通过Encodear压缩编码后，是否会有Patchify操作？Patchify本质上可看成对视频数据的二次压缩，目前的视频生成模型一般都包含这个操作，Sora将他们自己的做法称为"Spacetime Latent Patch"，此外Sora支持不同分辨率，不同长宽比的图片与视频生成，为了支持这个功能，在Patchify这步需要做些特殊的处理</p><p>DM主流网络结构一般会采用U-Net，Sora采用的是Transformer Diffusion Model。</p><p>Sora宣传时强调：可以支持不同分辨率，不同长宽比，不同时长的视频训练与生成。</p><p><strong>图像分辨率：</strong>分辨率一般和图片大小有关系，图片小分辨率就低一些，图片大清晰度或分辨率就高些。</p><p>目前做视频或图片生成的主流技术，为了方便内部处理（训练时Batch内数据的规则性），会把输入的图片或视频大小统一起来，对于处理不同大小的图片通过Crop操作，将输入大小统一，因此经过Crop数据训练的视频生成模型，生成的人体很容易看不完整，而使用不同的分辨率和长宽比回保持原始数据的所有信息，实体表达的完整性会好很多</p><p>Sora使用了图片与视频联合训练，这对于保证视频生成质量非常重要。因为带标注的<文本-图片>数据量多，各种风格的图片数据都包含，而带标视频数据少，很多情景要求下的数据都没有。</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241224135128547.png" alt="image-20241224135128547" /><figcaption aria-hidden="true">image-20241224135128547</figcaption></figure><p>可列出的关键技术：</p><ol type="1"><li>视频编码器-解码器</li></ol><p>​ 在支持“图片&amp;&amp;视频联合训练”、“视频长时一致性”这两个约束条件下，具体模型应该如何设计？</p><ol start="2" type="1"><li>Spacetime Latent Patch</li></ol><p>​ 在支持“图片&amp;&amp;视频联合训练”，“可变分辨率”这两个约束条件下，具体模型应该如何设计？</p><ol start="3" type="1"><li>基于Transformer的视频Diffusion Model</li></ol><p>​ 在支持“可变分辨率”约束条件下，具体模型应该如何设计？</p><ol start="4" type="1"><li>Diffusion Model阶段的长时一致性如何维护？</li></ol><h2 id="视频编码器-解码器从vae到tecotemporally-consistent-transformer">视频编码器-解码器：从VAE到TECO（Temporally Consistent Transformer）</h2><p>VAE模型可以分为 连续Latent和离散Latent模型。</p><p>VAE本身是连续Latent的，而离散Latent模型辩题众多，包括VQ-VAE和VQ-GAN。离散Latent火与GPT模型采用自回归生成离散Token模式有一定关联，使用离散Latent模型，较容易套到类似LLM的Next Token生成框架里，有望实现语言模型和图片、视频生成模型的一体化，如谷歌的VideoPoet.</p><p>Sora主干模型采用DM而非Next Token这种类LLM模型，DM的加噪去噪过程适合连续Latent空间进行，推断Sora采用连续Latent模式概率较大。</p><h3 id="teco">TECO</h3><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241220173447068.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ol type="1"><li>视频重建任务，用来训练视频Encoder-Decoder</li><li>使用Mask Git生成离散化的图像token，主要用于生成视频</li></ol><ul><li>在VAE编码阶段对Space和Time信息分别编码，且Time编码引入了极长的Long Time信息（所有历史信息）。要生成第i震视频，则Time编码会将1到第i-1帧之前的所有历史信息都融合到第i帧的时间编码里。</li><li>TECO在生成视频的长时一致性方面表现得很好</li></ul><p>对Sora来说，对TECO适应性地改造一下，能够把它在VAE阶段融合超长历史的能力吸收进来：</p><p>去掉离散化VAE，去掉MaskGit部分（用于训练模型能够Token by Token）地生成视频，只保留视频重建部分即可。</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-a1f82f5067a1a124c42e45cb363c860a_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li>对图片内容空间Latent编码，首帧单独处理，自己成为一组，可支持图片和视频联合训练；其它帧两帧一组，TECO的2帧类似于一个滑动窗口，窗口间有重叠，不存在多针压缩成一帧带来的信息损失问题，</li><li>视频帧分组侯，使用CNN 3D卷积可产生每帧图片对应的连续Latent，即Space Latent，主要编码图像的空间信息。之后使用Casual Temporal Transformer对时间信息进行编码，reshape后形成与Space Latent相同大小的高维表示，这部分是VAE的Time Latent，两者合并一起是这帧视频的VAE编码结果。</li></ul><h2 id="spacetime-latent-patch-spacetime-latent-patch的含义及navit">Spacetime Latent Patch: Spacetime Latent Patch的含义及NaVIT</h2><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-826a2c2b73f9b1aafeea3696a8c6a2a7_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><strong>单张图片Patchify</strong></p><p>本质上Patchify是对VAE压缩编码的二次压缩，在视频呢生成模型里很常见。如上图所示，对于VAE压缩后的“连续Latent”平面，可以设定一个 2∗2 大小的Patch，不重叠地扫描“连续Latent”平面，通常是接上一个MLP对 2∗2 的小正方形网格输入做个变换。这样的话，假设“连续Latent”本来大小是 8∗8 ，经过Patchify操作后，就形成了一个二次压缩的 4∗4 的Patch矩阵，然后可以通过线性化操作把Patch拉成一条直线，这是因为后面接的是Transformer，它需要线性的输入Patch形式</p><p>Patch Size越小，生成的视频质量越高。所以这里Sora采取 2∗2大小的Patch Size</p><p><strong>视频Patchify</strong></p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-044fb20db606bd135be3d0a646e9fa43_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>假设Sora在VAE阶段采用的是TECO，可以按照如上图做法：</p><p>每张图片有两个Patch矩阵，Space Latent(保留主要的是空间信息) + Time Latent(保留长时历史信息)。用一个Patch Size=2x2x2的Patch，把同一个图片的Space Latent和Time Latent合并，压缩为一个Patch矩阵</p><p>如下好处：</p><p>每张图片对应一个Patch矩阵，融合过程中既包含空间信息，也包含Long Time信息，信息保留充分。其次因首帧需要独立编码不能分组，因此该方案没有视频帧分组过程，支持图片&amp;&amp;视频联合训练。</p><p><strong>支持不同分辨率的视频（NAVIT）</strong></p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-83149c5aba9c89d3f38945b85029e027_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>基本思路：只要固定住Patch Size的大小，通过扫描不同分辨率的视频，自然就产生了不同分辨率或长宽比的Patch矩阵，然后把它们线性化即可。</p><p>Padding方案的问题：</p><ol type="1"><li>在训练模型的时候，一个Batch里被Padding无意义的占位符号浪费过多空间。而NaVIT不需要对每张图片进行Padding，最多在Batch末尾加少量Padding来填充到Batch的最大长度即可，可在一个Batch里放更多视频帧，极大增加模型的训练效率。</li><li>模型能支持的最大分辨率越高，Padding方法每张图片Padding了浪费的比例越高</li></ol><p>Sora最大可支持2048x2048的图片</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-c54875b718f4e5123addc5b77e104bda_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>将Patch线性化后会丢失Patch对应的位置信息，为了能够支持可变分辨率视频，对于每个Patch，需要特殊设计位置表征。</p><p>使用三维空间里的相对坐标，并学习对应的Position Embedding，<span class="math inline">\(X=2，Y=3，Z=3\)</span>视频时间维度的第三帧。假设在模型蓄念过程中学习每个坐标位置对应embedding，然后把三者embedding叠加，形成该Patch对应的Position Embedding。</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-eeee60bf108dff0d3fbfeacefd93e67c_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>归纳本阶段的技术方案如下：首先，很可能会对接TECO的VAE编码，使用 2∗2∗2 大小的Patch来合并每张图片的Space Latent以及Time Latent，每张图片被压成一个Spacetime Latent Patch矩阵。然后使用NaVIT方法来支持可变分辨率视频，最主要的改动是需要根据空间维度的两个坐标和时间轴坐标，来学习每个Patch在空间位置中对应三维空间相对位置坐标的Position Embedding。</p><h2 id="transformer-diffusion-model从diffusion-model原理到video-dits模型">Transformer Diffusion Model：从Diffusion Model原理到Video DiTs模型</h2><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-3dffa9ef7b5a38b8a50df37d08442d5b_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><ul><li>改造一：Transformer模型结构层面支持不同长宽比和分辨率的视频</li></ul><p>经过NaVIT改造，不同图片或视频帧的输入Patch是变长的，所以在Transformer阶段，我们需要引入Attention Mask机制，保证Transformer在做Local Spatial Attention的时候，属于某张图片的Patch只能相互之间看到自己这张图片内的其它Patch，但不能看到其它图片的内容；另外，因为这个引入的Attention Mask是针对输入Patch的，所以Transformer内的这个Local Spatial Attention模块一定在Transformer内部结构的最底层。</p><p>经过上述推导，我们可得出如上图所示的Transformer内部结构，它目前由两个子模块构成：最底层是Local Spatial Attention模块，主要计算图片或视频帧的空间信息，也就是对同一个视频帧内的各个Patch关系进行建模在它之上，有一个标准的MLP 模块，这个是Transformer模块做非线性映射所必需的。</p><p>面对的是变长Patch，具体采取什么技术手段才能实现针对变长Patch的Local Spatial Attention？</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-9ee6d197ca332b4f485be0e91f6153db_1440w.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>可能的解决方法：可采用“0/1 Attention Mask矩阵”来达成目标</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-4588e16976d5e7eca5a03af7772a329e_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>在我们上一步改造的Transformer结构里，加入一个Casual Time Attention子模块。Causal Time Attention模块的作用是在生成第i帧的时候，收集历史Time信息，也就是通过Attention让第i帧看到之前的比如k帧内容，这是用来维护生成视频的时间一致性的</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-b40ef414ff25a1330e83654247e6009d_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-66e9b44089371e475e2ca8bf083f70e8_r.jpg" /></p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-542a1acab40d4b4c0bf26d31225a03ef_1440w.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>Video DiTs的整个逻辑：</p><p>把噪音Patch线性化后，并入Prompt和Time Step条件，一起作为Transformer的输入。Transformer内部由三个子模块构成：Local Spatial Attention模块负责收集视频帧空间信息；Causal Time Attention模块负责收集历史时间信息；MLP模块负责对时间和空间信息通过非线性进行融合</p><h2 id="sora的long-time-consistency可能策略暴力美学还是fdm">Sora的Long Time Consistency可能策略：暴力美学还是FDM？</h2><p>在Transformer Diffusion Model阶段维护“长时一致性”策略方面，感觉FDM（Flexible Diffusion Modeling）方法是种简洁有效的思路。FDM（可参考：Flexible Diffusion Modeling of Long Videos）提出了两种Time Attention改进模型，</p><h2 id="sora的训练过程与技巧合成数据两阶段训练及双向生成">Sora的训练过程与技巧：合成数据、两阶段训练及双向生成</h2><p>所有文生视频模型，本质上都是有监督学习，是需要大量高质量标注好的&lt;文本，视频&gt;成对数据来训练的，它们不是类似LLM的那种自监督学习那样，无需标注数据。</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-628c10c2fcbd479a921e4b484f451f53_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>一般VAE是独立训练的，收集大量的图片或视频数据后，通过图片或视频重建的训练目标，可以得到对应的“视觉编码器-解码器”。此部分训练是自监督学习，不需要标注数据</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-eb89c3b6df7a213e6ff4aacca466af91_r.jpg" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>第二阶段是包括Diffusion Model在内整个模型的训练，这一阶段训练过程中，一般前一阶段训练好的Encoder-Decoder会冻结模型参数，不随着这步骤的训练数据发生变动，包括Text Encoder也是利用现成的比如CLIP，也会类似地冻结住模型参数。所以这部分训练主要涉及Spacetime Latent Patch对应的Position Embedding，以及预测噪音的基于Transformer的Diffusion Model的训练。</p><p>参考 ：https://zhuanlan.zhihu.com/p/687928845</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>VisualGLM详解</title>
    <link href="/2024/12/19/VisualGLM%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/19/VisualGLM%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="visualglm-6b">VisualGLM-6B</h1><h1 id="前置内容">前置内容</h1><p>多模态预训练思路一：（CogView思路）</p><ul><li>图像作为一种特殊的语言</li></ul><p>多模态预训练思路二：（VisualGLM思路）</p><ul><li>图像对齐到预训练语言模型</li></ul><p>以BERT/GPT为代表的自回归模型展现出强大的理解生成能力 如何将这种成功“复制”到视觉/跨模态领域? - 以DALL-E、CogView、BEiT等为代表的模型的思路为： - 将图像当成一种语言进行与训练 - 离散化token 优势： 图像为一等公民，同时及逆行图像生成和理解 劣势： 离散化有损，降低token利用率，需要&gt;1000tokens来描述一张256分辨率的图</p><p>实际上人类只对少量的视觉语义信息感兴趣，如何提升效率，充分利用语言模型？ BLIP2提供了一种思路： 将图像特征对齐到预训练语言模型 优势：充分利用语言包模型 无缝接合原有的多轮对话能力 （VQA等） 劣势：提取图像语义特征损失底层信息</p><p>Q-former会损失他认为不重要的信息 如地板颜色等</p><p>VisualGLM-6B 如何让ChatGLM拥有图像识别的能力？ - 通用领域 - 中英双语 - 与正常问答顺畅融合</p><p>基本思路： - 通过中间模块构建起预训练视觉和语言模型的桥梁 - 中英双语图文数据大规模预训练 - 高质量指令数据微调 - 与纯语言累死的技术方案 - BLIP2、MiniGPT4等工作类似思路</p><h2 id="visualglm-6b训练">VisualGLM-6B训练</h2><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241219204450866.png" alt="image-20241219204450866" /><figcaption aria-hidden="true">image-20241219204450866</figcaption></figure><h3 id="模型架构">模型架构：</h3><ul><li>ViT+QFormer+ChatGLM-6B</li><li>几乎冻结ViT和ChatGLM参数，防止灾难性遗忘（Blip2中说完全冻结大语言模型，但实际中发现还是得少量改动视觉模型和语言模型，有些信息可能没有办法学到）</li><li>预训练： QFormer和ViT Lora进行学习</li></ul><p>预训练过程中用了一个很大的语料，pretrained Vit可能都没有见过，预训练Vit需要学习一些新的视觉概念，放开一部分Vit的参数。</p><ul><li>微调：QFormer和ChatGLM Lora进行学习</li></ul><p>微调的过程中使用的是高质量的图文数据，高质量是指回答过程中遵循一定规律，需要让语言模型学会遵循这种回答的方法，这时需要放开语言模型的参数。</p><p>预训练与微调过程中对视觉模型与大语言模型的少量参数的放开学习会比BLIP2完全冻结的效果要更好。</p><h3 id="训练目标">训练目标：</h3><ul><li>自回归损失（根据图像生成正确的文本）</li><li>对比损失（输入ChatGLM的视觉特征与对应文本的语义特征对齐）</li></ul><h3 id="训练数据">训练数据</h3><ul><li>CogView工作基类的30M中文图文对</li><li>精选LAION+CC12M的100M英文图文对</li><li>来自其他工作和数据集的视觉问答指令微调数据集</li><li>自己构建的高质量视觉问答指令微调数据集</li></ul><p>quant 4 bit 模型效果几乎没有什么损失</p><p>多模态任务分布广、种类多，预训练往往不能面面俱到，可以通过小样本微调，如使用20张标注图增强模型回答背景问题的能力。</p><p>微调数据格式：</p><p>"<img>" + img_embedding+"</img>问"+prompt+"："+label</p><p>微调数据json文件如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/2p.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是蒙蒙细雨。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/pig.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是是虚化的。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/meme.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是蓝色的木质地板。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/passport.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是棕黄色木质桌子。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/tower.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是黄昏的天空、云彩和繁华的城市高楼。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/rub.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是太阳、大树、蓝天白云。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/push.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是蓝天和沙漠。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/traf.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是城市街道。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/music.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是一个音乐混音器。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/pattern.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是小区的楼房和街道。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/rou.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是大理石桌子和一个盘子。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/katong.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是绿色的草地。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/man.jpg&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是城市的街道和高楼。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/kobe.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是虚化的观众席。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/panda.jpg&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是纯白的。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/titan.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是一座雕像。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/woman.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是纯蓝的。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/ghost.jpg&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是一个房间。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/justice.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是天空和阳光。&quot;</span><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;img&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fewshot-data/tianye.png&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;prompt&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景里有什么内容？&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;这张图片的背景是金黄的田野。&quot;</span><span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Stable-diffusion详解</title>
    <link href="/2024/12/19/Stable-diffusion%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/19/Stable-diffusion%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="ddpm">DDPM</h1><h2 id="正向过程">正向过程</h2><p>前向过程：给定真实图片样本$x_0 q(x) $ ,前向过程通过T次累计对其添加高斯噪声，每一步大小是由一系列的高斯分布方差的超参数<span class="math inline">\(\{ \beta_t \in (0,1)_{t=1} ^T \}\)</span>控制。前向过程由于每个时刻t只与t-1时刻有关，所以可看作马尔科夫过程： <span class="math display">\[q(x_t | x_{t-1})= \mathcal{N}(x_t;\sqrt{1-\beta_t} x_{t-1},\beta_t I )\]</span> 整个前向过程是一个后验估计，被表示为：（根据联合概率密度+马尔科夫链） <span class="math display">\[q(x_{1:T} |x_0) =\prod _{t=1} ^T  q(x_t|x_{t-1})\]</span></p><ul><li><p><span class="math inline">\(\beta_t\)</span>为方差</p></li><li><p><span class="math inline">\(\sqrt{1-\beta_t}\)</span>为均值，<span class="math inline">\(\alpha_t=1-\beta_t\)</span></p></li></ul><p>重参数技巧</p><p>从高斯分布$z (z;<em>,</em>^2 I) $ 采样<span class="math inline">\(z\)</span> 可写成： <span class="math display">\[z=\mu_\theta +\sigma_\theta \odot  \epsilon , \epsilon \in \mathcal{N}(0,I)\]</span> 因此<span class="math inline">\(q(x_t | x_{t-1})= \mathcal{N}(x_t;\sqrt{1-\beta_t} x_{t-1},\beta_t I )\)</span> 可写成： <span class="math display">\[x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} \epsilon_{t-1}\]</span> 可推出： <span class="math display">\[q(x_t | x_0)=\mathcal{N} (x_t; \sqrt{\bar \alpha_t} x_0,(1-\bar \alpha_t  )I )\]</span> 其中<span class="math inline">\(\bar \alpha_t = \alpha_t \alpha_{t-1} ... \alpha_3 \alpha_2 \alpha_1\)</span></p><h2 id="逆向过程去噪生成">逆向过程（去噪，生成）</h2><p>属于Likelihood-based Model ，优化目标是最大化真实数据分布的似然估计<span class="math inline">\(p_\theta(x_0)\)</span> 其中<span class="math inline">\(\theta\)</span> 表示用用一个神经网络来学习</p><p>将每一步逆向过程建模成<span class="math inline">\(p_\theta(x_{t-1}|x_t)\)</span> 记作<span class="math inline">\(p_\theta(x_{t-1} |x_t) =\mathcal{N}(x_{t-1};\mu_\theta(x_t,t),\Sigma_\theta(x_t,t) )\)</span></p><p>整个逆向过程表示成一个联合概率分布$p_(x_{0:T})=p(x_T)<em>{t-1}^T p</em>(x_{t-1}|x_t) $</p><p>优化目标为： <span class="math display">\[\begin{align*}&amp;  \arg\min\limits_{\theta} D_{KL} (q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1} |x_t))\\= &amp; \arg\min\limits_{\theta}  \frac{1}{2\sigma_q^2(t)} \frac{(1-\alpha_t)^2}{(1-\bar \alpha_t) \alpha_t } [||f_\theta(x_t,t)-\epsilon_t  ||_2^2]   \end{align*}\]</span> 因此求： <span class="math display">\[\nabla _\theta || \epsilon-\epsilon_\theta (\sqrt{\bar \alpha_t} x_0 + \sqrt{1-\bar \alpha_t} \epsilon,t )   ||^2\]</span> 训练后，进行采样</p><p>采样则满足： <span class="math display">\[x_{t-1} =\frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1-\alpha_t}{\sqrt{1-\bar \alpha_t}} \epsilon_\theta(x_t,t)) +\sigma_t z\]</span></p><p>前向扩散： <span class="math display">\[q(x_t | x_{t-1})= \mathcal{N}(x_t;\sqrt{1-\beta_t} x_{t-1},\beta_t I )\]</span> 反向生成： <span class="math display">\[q(x_{t-1}|x_t,x_0)=\mathcal{N}(x_t,;\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{(1-\alpha_t)}{\sqrt{(1-\bar \alpha_t)}} \epsilon_t) ,\beta_t \frac{1-\bar \alpha_{t-1}}{1-\bar \alpha_t}  I)\]</span></p><h2 id="训练过程">训练过程</h2><p><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241209203835039.png" /></p><ol type="1"><li>从数据集中随机选择一个训练样本，从K个噪声量级随机抽取一个时间t，并编码</li><li>通过$x_t = x_0 +  $ 将高斯噪声添加到图片中得到 Noisy image,其中$(0,1),_t=1-_t ,<em>t=</em>{i=1} ^t _i $</li><li>将Noisy image和Time step embedding一起加入到UNet中</li></ol><p><strong>ResNetBlock模块</strong> 借助resnet的残差结构，让网络能够构建更深的同时，将Time Embedding信息嵌入模型</p><p><strong>CrossAttention模块</strong> 接受ResNetBlock模块的输出和Clip text Encoder编码后的Context Embedding<strong>其不改变输入输出的尺寸，只在图片对应的位置上融合了语义信息</strong></p><p><strong>BasicTransformer Block</strong>模块在Cross Attention模块的基础上，增加了self Attention子模块和Feedforward子模块共同组成。</p><p><strong>Self Attention</strong> 可以将输入图像的不同部分进行交互，实现特征的整合和全局上下文的引入，让模型建立捕捉图像全局关系的能力，有助于模型理解不同位置不同像素之间的依赖关系，以更好地理解图像的语义。</p><p><strong>Spatial Transformer模块</strong> 在BasicTransformer Block模块基础上，加入GroupNorm和两个卷积层组成Spatial Transformer模块</p><p><strong>在生成式模型中，GroupNorm的效果一般会比BatchNorm更好</strong>，生成式模型通常比较复杂，因此需要更稳定和适应性强的归一化方法，SiLU在深层模型上的效果优于 ReLU。可以看做是平滑的ReLU激活函数</p><h1 id="stable-diffusion">Stable Diffusion</h1><p><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/955541bd067709d79bc2f08043a81b1.png" /></p><ul><li><p>LDM=VAE+DDPM</p></li><li><p>LDM是在语义空间做diffusion,DDPM是在像素空间做diffusion</p></li><li><p>LDM有更多模态的融入：1. 类别融入 2. 文本融入</p></li></ul><p>Diffusion Model模型的训练和推理非常耗时</p><p>语义空间为什么work？</p><ol type="1"><li>数字图像的大多数像素都用来描述细节</li><li>在像素空间做DM，存在大量计算冗余</li></ol><p>如何控制图片的生成？</p><ul><li><p>类别控制</p><ul><li><p>Classifier Guidance</p><p>​ 单独训练一个分类器，采样是加入分类器的信息</p><p>​ 训练一个什么样的分类器？</p><p>​ 输入输出是什么？</p><p>​ 采样时怎么加入分类信息？</p></li><li><p>Classifier Free</p></li></ul></li><li><p>文本控制</p><ul><li>Text Guidance</li></ul></li></ul><p><strong>Classifier Guidance</strong></p><p>给定y的条件下，求<span class="math inline">\(p(x_{t-1}|x_t)\)</span> $$ <span class="math display">\[\begin{align*}p(x_{t-1}|x_t,y) &amp;=\frac{p(x_t|x_{t-1},y) p(x_{t-1}|y)  }{p(x_t|y)}\\ &amp;=\frac{ p(x_t|x_{t-1})p(x_{t-1}|y)  }{p(x_t|y)}\end{align*}\]</span> $$ 为什么UNet中图像信息与Q矩阵结合</p><p>图像生成任务的目的是从噪声中生成图像，Q代表的是当前图像特征的上下文，因此在每个生成步骤中，图像特征会被逐步调整和改进，使得不同部分需要逐步去适应文本信息，生成符合文本描述的图像。</p><p>文本嵌入提供了如何生成图像的语义指导，图的上下文Q需要根据这个语义指导进行调整</p><p>交互核心：图像生成与文本引导</p><p>跨模态注意力的核心目的，是让模型通过结合文本描述的指导信息来生成图像，图像特征图Q是输入的上下文，文本嵌入K和V则提供了语义上的指引，</p><p><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241209145028295.png" /></p><p>在进行Stable Diffusion模型训练时，VAE部分和CLIP部分是冻结的</p><h2 id="stable-diffusion-文生图">stable diffusion 文生图</h2><ul><li>使用CLIP Text Encodear模型将输入的人类文本信息进行编码，输出特征矩阵；</li><li>输入文本信息，再用random函数生成一个高斯噪声矩阵，作为Latent Feature的替代输入到SD模型的图像优化模块中</li><li>将图像优化模块进行优化迭代后的Latent Feature输入到图像解码器中，将Latent Feature重建成像素级图</li></ul><p><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241209151432905.png" /></p><h2 id="stable-diffusuion图生图">Stable Diffusuion图生图</h2><ul><li>在输入文本信息进行编码的同时，将原图片通过图像编码器（VAE Encoder）生成Latent Feature(隐空间特征)作为输入</li><li>将上述信息输入到SD模型的"图像优化模块中"</li><li>将图像优化模块进行优化迭代后的Latent Feature输入到图像解码器中，将Latent Feature重建成像素级图</li></ul><p><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241209152033425.png" /></p><p>文生图与图生图的核心都是图像优化模块，其输入都是文字+图片，输出都是一张经过优化后的图片；但温升图任务中图像优化模块的输入是一张随机生成的噪声图</p><p><strong>图像优化模块</strong> 由U-Net网络和一个Schedule算法共同组成</p><ul><li>U-Net网络负责预测噪声，不断优化生成过程，在预测噪声的同时不断注入文本语义信息</li><li>schedule算法对每次U-Net预测的噪声进行优化处理（动态调整预测的噪声，控制U-Net预测噪声的强度），从而统筹生成过程的进度</li><li>SD中，U-Net迭代优化部署大概是50或100</li></ul><h2 id="stable-diffusion中u-net的训练过程与损失函数"><strong>Stable Diffusion中U-Net的训练过程与损失函数</strong></h2><p>在我们进行Stable Diffusion模型训练时，VAE部分和CLIP部分都是冻结的，所以说官方在训练SD系列模型的时候，训练过程一般主要训练U-Net部分 <span class="math display">\[L_{SD} = \mathbb{E}_{x_0,\epsilon \sim \mathcal{N}(0,I),t}[||\epsilon - \epsilon_\theta(\sqrt{\bar \alpha_t}x_0+\sqrt{1-\bar \alpha_t}\epsilon,t,c)||^2]\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion Transformer详解</title>
    <link href="/2024/12/15/Diffusion_Transformer%E8%AF%A6%E8%A7%A3/"/>
    <url>/2024/12/15/Diffusion_Transformer%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="vit架构及介绍">Vit架构及介绍</h1><p>vit是谷歌提出的把Transformer应用到图像分类的模型，论文的最核心结论是：<strong>当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果</strong></p><p>当训练数据集不够大的时候，Vit通常比同等大小的Resnets要差一些，因为Transformer和CNN相比缺少归纳偏置，即一种先验知识。（CNN具有两种归纳偏置，1.局部性，图片上相邻区域具有相似的特征；2.平移不变性。CNN具有上述两种归纳偏置，具有先验信息，相对少的数据就可以学到一个比较好的模型）</p><h2 id="vit的架构">Vit的架构</h2><p>Vit将输入图片分为多个patch(16x16)，再将每个patch投影为固定长度的向量送入Transformer，后续encoder的操作和原始Transformer中完全相同，但需要对图像进行分类，故而在输入序列中加入一个特殊的token。该token对应的输出即为最后的类别预测</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218225601439.png" title="vit架构图" alt="vit架构图" /><figcaption aria-hidden="true">vit架构图</figcaption></figure><h2 id="步骤">步骤</h2><ul><li><p>patch embedding：如输入的图片大小为224x224，将图片分为固定大小的patch，patch大小为16x16，每张图片会生成196个patch，每个patch的维度为16x16x3，输入通过线性投影层后维度依然为196x768。<strong>这里需要加一个特殊的字符cls</strong>，则最终维度为<strong>197 x 768</strong> 。将视觉问题转化为了一个seq2seq问题。</p></li><li><p>positional encoding ：Vit需要加入位置编码。位置编码可理解成一张表，共N行，与输入序列长度相同(197)，每行代表一个向量，向量维度与输入序列embedding的维度相同（768）。编码的操作是sum</p></li><li><p>LN/multi-head attention/LN 维度依然是197x768</p></li><li><p>MLP：将维度放大，再缩回去，197x768-&gt;197x3072-&gt;197x768。</p><p>一个Block之后维度保持不变，因此可以堆叠多个，最后将特殊字符cls对应的输出<span class="math inline">\(z_L^0\)</span>作为encoder的最终输出。</p><p>（另一种做法：不加cls字符，对所有tokens的输出做一个平均）</p></li></ul><h2 id="关于positional-encoding">关于positional encoding</h2><ul><li>1-D 位置编码， 1-9</li><li>2-D 位置编码， 11,12，... ,33</li></ul><p><strong>奇怪现象：1-D，2-D，不适用位置编码，性能非常接近，即使不使用位置编码，模型的性能损失也不是很大。</strong></p><p>解释：</p><ol type="1"><li><p>Patch-Level Processing(最主要原因)</p><p>VIT模型将图像分为多个小块，并将每个小块作为一个单独的输入。相比于处理像素级别的信息，这些小块之间的相对位置更容易被模型理解和利用。通过这些小块进行自注意力机制，能够捕捉到他们之间的空间关系</p></li><li><p>transformer的自注意力机制：</p></li></ol><p>Vit的Transformer 层具有强大的自注意力机制，可以在一个序列中的不同位置之间建立复杂的关联</p><ol start="3" type="1"><li>模型的学习能力。</li></ol><p>优点：</p><ol type="1"><li>全局感知特性：ViT能够在全局范围内捕捉特征和上下文关系，源于它的自注意力机制，能够理解图像的全局上下文</li><li>数据效率：ViT在大规模数据集上表现优秀，这使得ViT可以更有效利用那些包含超大量样本的数据集</li><li>数据通用性：ViT借鉴了transformer模型，具有很强的迁移学习能力，更有利于后续的模态融合。</li></ol><p>缺点：</p><ol type="1"><li>需要大量数据：ViT要活的良好性能，需要大量训练数据，对于小规模数据集不如CNN</li><li>计算成本高：ViT的计算和内存需求比传统的CNN大，特别在高分辨率图像中，自注意力的计算复杂度会随着像素数量的增加而增加</li><li>可解释性差：尽管Transformer结构在很多任务上有很好的表现，但它的可解释性往往较差。这是因为ViT模型学到的全局信息和关系，可能并不直观或容易理解，这给模型的理解和优化带来了困难。</li></ol><p><strong>ViT适合在哪些场景下应用：</strong></p><ol type="1"><li><strong>大规模图像识别</strong>：ViT在数据丰富的环境下表现尤其突出，因此适合需要在大规模数据集上进行复杂图像识别任务的场景，例如图像分类、物体识别等。</li><li><strong>高清图像处理</strong>：利用它的全局感知和高分辺率处理能力，ViT适用于高清图像的分析，如医学成像、卫星图像分析等。</li><li><strong>迁移学习任务</strong>：ViT的模型通用性使它在迁移学习场景中表现良好，可以应用于那些与预训练数据领域相近的任务上。</li><li><strong>多模态学习</strong>：因为Transformer架构的灵活性，ViT可以与其他模式的Transformer（如用于文本或音频的Transformer）结合，进行多模态学习，比如图像-文本匹配等。</li><li><strong>科学研究</strong>：在需要精确理解图像全局上下文的科学研究中，比如地理信息系统(GIS)、天体物理学等，ViT由于其全局感知特性也非常有用。</li></ol><h1 id="dit-架构及介绍">Dit 架构及介绍</h1><p>《Scalable Diffusion Models with Transformers》</p><h2 id="patchify过程">Patchify过程</h2><ul><li><p>图像的大小为<span class="math inline">\(I \times I \times c\)</span> ，将图像Patchify，并经过Linear Embedding，变成T个d维的tokens，经过Linear Embedding，得到T个d维的tokens。</p></li><li><p>Patchify后，作者将标准的基于Vit频率的位置编码（sin-cosine）应用到输入的tokens上</p><p>作者在Dit design space里使用p=2,4,8，token T的数量由Patch大小p决定，满足<span class="math inline">\(T=(I/p)^2\)</span></p></li></ul><h2 id="dit-block设计">Dit Block设计</h2><p>Patchify后，输入的tokens开始进入一系列的Transformer Block中，Diffusion Model会额外处理条件信息，如噪声时间步长t，标签c,自然语言text。</p><p>作者探索了4种不同类型的Transformer Block，以不同的方式处理条件输入，这些设计对标准Vit Block进行微小修改。</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/v2-5efe86e40ab87a05d33a2e8c75539f72_r.jpg" title="Dit 框架图" alt="Dit 框架图" /><figcaption aria-hidden="true">Dit 框架图</figcaption></figure><ul><li>In-Context Conditioning</li></ul><p>带条件输入的情况，该方法将时间步长t，类标签c作为两个额外的token附加到输入的序列中。</p><p>作者将其视为与图像的token无区别，允许Dit使用标准的Vit Block（无修改），经过最后一个Block后，删除条件token即可。</p><p>In-Context Conditioning带来的额外 GFLOPs 微不足道。</p><ul><li>Cross-Attention Block</li></ul><p>该方法将时间步长t和类标签c的Embedding连接成一个长度为2的Sequence，且与image token序列分开，该方法给Transformer Block添加一个Cross-Attention块。</p><p>Cross-Attention Block带来的额外GFLOPs开销大约15%。</p><ul><li>Adaptive Layer Norm（adaLN）Block</li></ul><p>adaLN Block遵循GAN中的自适应归一化层，探索这个在扩散模型里是否适用。本文作者并未学习缩放和移位参数<span class="math inline">\(\gamma\)</span> 和<span class="math inline">\(\beta\)</span>，而是改用噪声时长t和类标签c得到。</p><p>Adaptive Layer Norm（adaLN）Block带来的额外的 GFLOPs 是最少的。</p><ul><li>adaLN-Zero Block</li></ul><p>有监督学习中，对于每个Block的第一个Batch Norm操作的缩放因子进行Zero-Initialization可以加速其大规模训练。基于U-Net的扩散模型使用类似的初始化策略，对每个Block的第一个卷积进行Zero-Initialization。本文作者做了一些改进：回归计算缩放参数<span class="math inline">\(\gamma\)</span>和移位参数<span class="math inline">\(\beta\)</span>外，还回归缩放系数<span class="math inline">\(\alpha\)</span>。</p><p>作者初始化MLP使其输出的缩放系数<span class="math inline">\(\alpha\)</span> 全部为0，使得Dit Block初始化为恒等函数（Identity Function）。</p><p>adaLN-Zero Block带来的额外的GFLOPs可忽略不计</p><h2 id="transformer-decoder">Transformer Decoder</h2><p>在最后一个Dit Block后，需要将image tokens的序列解码为输出噪声以及对角的协方差矩阵的预测结果，且这两个输出的形状都与原始空间输入一致。作者在该环节使用标准的线性解码器，将每个token线性解码为<span class="math inline">\(p \times p \times 2C\)</span>的张量，其中<span class="math inline">\(C\)</span>是空间输入中到Dit的通道数，最后将解码的tokens重新排列到其原始空间布局中，得到预测的噪声和协方差。</p><h2 id="dit-训练策略">Dit 训练策略</h2><h3 id="训练设置">训练设置</h3><p>作者在ImageNet 数据集上训练class-conditional latent Dit模型，分辨率设为256x256和512x512，对最后的Linear Layer使用Zero Initialization,其余使用Vit的标准权重初始化方案。优化器使用AdamW</p><h3 id="扩散模型配置">扩散模型配置</h3><p>Dit其他组件，作者中Stable Diffusion预训练好的Variational AutoEncoder模型，下采样率是8，给定256x246x3的输入图片x，得到的编码结果z尺寸为32x32x4，从扩散模型中Sample出一个新的latent结果后，作者使用VAE decoder，将其解码回pixel:<span class="math inline">\(x=D(z)\)</span></p><h2 id="代码解析">代码解析</h2><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218214457127.png" title="Dit 模块图" alt="Dit 模块图241218214457127" /><figcaption aria-hidden="true">Dit 模块图241218214457127</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiT</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Diffusion model with a Transformer backbone.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_size=<span class="hljs-number">32</span>,</span><br><span class="hljs-params">        patch_size=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">        in_channels=<span class="hljs-number">4</span>,</span><br><span class="hljs-params">        hidden_size=<span class="hljs-number">1152</span>,</span><br><span class="hljs-params">        depth=<span class="hljs-number">28</span>,</span><br><span class="hljs-params">        num_heads=<span class="hljs-number">16</span>,</span><br><span class="hljs-params">        mlp_ratio=<span class="hljs-number">4.0</span>,</span><br><span class="hljs-params">        class_dropout_prob=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">        num_classes=<span class="hljs-number">1000</span>,</span><br><span class="hljs-params">        learn_sigma=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.learn_sigma = learn_sigma<br>        self.in_channels = in_channels<br>        self.out_channels = in_channels * <span class="hljs-number">2</span> <span class="hljs-keyword">if</span> learn_sigma <span class="hljs-keyword">else</span> in_channels<br>        self.patch_size = patch_size<br>        self.num_heads = num_heads<br><br>        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=<span class="hljs-literal">True</span>)<br>        self.t_embedder = TimestepEmbedder(hidden_size)<br>        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)<br>        num_patches = self.x_embedder.num_patches<br>        <span class="hljs-comment"># Will use fixed sin-cos embedding:</span><br>        self.pos_embed = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>, num_patches, hidden_size), requires_grad=<span class="hljs-literal">False</span>)<br><br>        self.blocks = nn.ModuleList([<br>            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(depth)<br>        ])<br>        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)<br>        self.initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># Initialize transformer layers:</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_basic_init</span>(<span class="hljs-params">module</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear):<br>                torch.nn.init.xavier_uniform_(module.weight)<br>                <span class="hljs-keyword">if</span> module.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    nn.init.constant_(module.bias, <span class="hljs-number">0</span>)<br>        self.apply(_basic_init)<br><br>        <span class="hljs-comment"># Initialize (and freeze) pos_embed by sin-cos embedding:</span><br>        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-<span class="hljs-number">1</span>], <span class="hljs-built_in">int</span>(self.x_embedder.num_patches ** <span class="hljs-number">0.5</span>))<br>        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).<span class="hljs-built_in">float</span>().unsqueeze(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># Initialize patch_embed like nn.Linear (instead of nn.Conv2d):</span><br>        w = self.x_embedder.proj.weight.data<br>        nn.init.xavier_uniform_(w.view([w.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>]))<br>        nn.init.constant_(self.x_embedder.proj.bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Initialize label embedding table:</span><br>        nn.init.normal_(self.y_embedder.embedding_table.weight, std=<span class="hljs-number">0.02</span>)<br><br>        <span class="hljs-comment"># Initialize timestep embedding MLP:</span><br>        nn.init.normal_(self.t_embedder.mlp[<span class="hljs-number">0</span>].weight, std=<span class="hljs-number">0.02</span>)<br>        nn.init.normal_(self.t_embedder.mlp[<span class="hljs-number">2</span>].weight, std=<span class="hljs-number">0.02</span>)<br><br>        <span class="hljs-comment"># Zero-out adaLN modulation layers in DiT blocks:</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Zero-out output layers:</span><br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.bias, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unpatchify</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        x: (N, T, patch_size**2 * C)</span><br><span class="hljs-string">        imgs: (N, H, W, C)</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        c = self.out_channels<br>        p = self.x_embedder.patch_size[<span class="hljs-number">0</span>]<br>        h = w = <span class="hljs-built_in">int</span>(x.shape[<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>)<br>        <span class="hljs-keyword">assert</span> h * w == x.shape[<span class="hljs-number">1</span>]<br><br>        x = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], h, w, p, p, c))<br>        x = torch.einsum(<span class="hljs-string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)<br>        imgs = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], c, h * p, h * p))<br>        <span class="hljs-keyword">return</span> imgs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, t, y</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Forward pass of DiT.</span><br><span class="hljs-string">        x: (N, C, H, W) tensor of spatial inputs (images or latent representations of images)</span><br><span class="hljs-string">        t: (N,) tensor of diffusion timesteps</span><br><span class="hljs-string">        y: (N,) tensor of class labels</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x = self.x_embedder(x) + self.pos_embed  <span class="hljs-comment"># (N, T, D), where T = H * W / patch_size ** 2</span><br>        t = self.t_embedder(t)                   <span class="hljs-comment"># (N, D)</span><br>        y = self.y_embedder(y, self.training)    <span class="hljs-comment"># (N, D)</span><br>        c = t + y                                <span class="hljs-comment"># (N, D)</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            x = block(x, c)                      <span class="hljs-comment"># (N, T, D)</span><br>        x = self.final_layer(x, c)                <span class="hljs-comment"># (N, T, patch_size ** 2 * out_channels)</span><br>        x = self.unpatchify(x)                   <span class="hljs-comment"># (N, out_channels, H, W)</span><br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_with_cfg</span>(<span class="hljs-params">self, x, t, y, cfg_scale</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb</span><br>        half = x[: <span class="hljs-built_in">len</span>(x) // <span class="hljs-number">2</span>]<br>        combined = torch.cat([half, half], dim=<span class="hljs-number">0</span>)<br>        model_out = self.forward(combined, t, y)<br>        <span class="hljs-comment"># For exact reproducibility reasons, we apply classifier-free guidance on only</span><br>        <span class="hljs-comment"># three channels by default. The standard approach to cfg applies it to all channels.</span><br>        <span class="hljs-comment"># This can be done by uncommenting the following line and commenting-out the line following that.</span><br>        <span class="hljs-comment"># eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]</span><br>        eps, rest = model_out[:, :<span class="hljs-number">3</span>], model_out[:, <span class="hljs-number">3</span>:]<br>        cond_eps, uncond_eps = torch.split(eps, <span class="hljs-built_in">len</span>(eps) // <span class="hljs-number">2</span>, dim=<span class="hljs-number">0</span>)<br>        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)<br>        eps = torch.cat([half_eps, half_eps], dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> torch.cat([eps, rest], dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>如上图所示，x（images or latent representations of images）先经过PatchEmbed，与位置编码相加得到；y的embedding与t的embedding直接相加得到c，此后x与c经过若干个Dit Block。</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241218214935006.png" title="Dit Block图" alt="Dit Block图241218214935006" /><figcaption aria-hidden="true">Dit Block图241218214935006</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DiTBlock</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size, num_heads, mlp_ratio=<span class="hljs-number">4.0</span>, **block_kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=<span class="hljs-literal">True</span>, **block_kwargs)<br>        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        mlp_hidden_dim = <span class="hljs-built_in">int</span>(hidden_size * mlp_ratio)<br>        approx_gelu = <span class="hljs-keyword">lambda</span>: nn.GELU(approximate=<span class="hljs-string">&quot;tanh&quot;</span>)<br>        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=<span class="hljs-number">0</span>)<br>        self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">6</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(<span class="hljs-number">6</span>, dim=<span class="hljs-number">1</span>)<br>        x = x + gate_msa.unsqueeze(<span class="hljs-number">1</span>) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))<br>        x = x + gate_mlp.unsqueeze(<span class="hljs-number">1</span>) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><ol type="1"><li>adaLN-Zero Block 是 <strong>adaptive LayerNorm-Zero</strong> Block 的简称。它是在传统的 Transformer 模块中引入了自适应的层归一化（Layer Normalization）机制，通过将条件信息（如扩散步数、标签等）动态地融入网络参数中，来提高生成模型的效果。它特别结合了 <strong>AdaLN</strong> 和 <strong>Zero Initialization</strong>，使得网络可以更好地适应不同的条件输入。</li></ol><p>在传统 Layer Normalization 的基础上，<strong>AdaLN</strong> 使用外部条件（例如时间步 <code>t</code>）来动态调整 LayerNorm 的尺度和偏移参数。具体表达为：</p><p><span class="math display">\[AdaLN(h;\gamma(t),\beta(t))=\gamma(t) \cdot \frac{h-\mu}{\sigma} +\beta(t)\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">modulate</span>(<span class="hljs-params">x, shift, scale</span>):<br>    <span class="hljs-keyword">return</span> x * (<span class="hljs-number">1</span> + scale.unsqueeze(<span class="hljs-number">1</span>)) + shift.unsqueeze(<span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(<span class="hljs-number">6</span>, dim=<span class="hljs-number">1</span>)<br>        x = x + gate_msa.unsqueeze(<span class="hljs-number">1</span>) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))<br>        x = x + gate_mlp.unsqueeze(<span class="hljs-number">1</span>) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))<br>        <span class="hljs-keyword">return</span> x<br>  <br><br>self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">6</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br></code></pre></td></tr></table></figure><p>这里的<strong>6组可学习的参数</strong>由self.adaLN_modulation线性层，再通过chunk(6, dim=1)得到</p><p>这里的Multi-Head Self-Attention和Pointwise Feedforward分别为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=<span class="hljs-literal">True</span>, **block_kwargs)<br><br>self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>它称为 <strong>Pointwise</strong>，是因为它的操作不考虑不同位置之间的交互，仅在单个位置的向量上进行变换。</p><ul><li>Pointwise Feedforward Network 是 Transformer 架构中常见的模块，它对输入序列中的每个位置（token）的特征向量 <strong>独立</strong> 进行相同的前向传播运算（升维 -&gt; 激活 -&gt; 降维）</li><li>普通的 Feedforward Network 是标准的全连接神经网络，它将整个输入向量映射到输出向量，通过非线性激活函数引入非线性特性。</li></ul><ol start="2" type="1"><li>Zero Initialization</li></ol><p><strong>Zero Initialization</strong> 的关键思想是在模块开始训练时，将残差块的输出初始化为零，以确保初始时模型的输入和输出保持一致。这种做法有两个好处：</p><ol type="1"><li><strong>稳定训练</strong>：避免训练初期网络过早出现大幅度的变化。</li><li><strong>逐渐学习特征</strong>：残差分支在训练过程中逐渐学习有用的特征，从零开始优化。</li></ol><p>在实现上，网络的残差连接的输出会被一个动态缩放因子初始化为零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_weights</span>(<span class="hljs-params">self</span>):<br>    ...<br>  <br><span class="hljs-comment"># Zero-out adaLN modulation layers in DiT blocks:</span><br>        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> self.blocks:<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>            nn.init.constant_(block.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Zero-out output layers:</span><br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.adaLN_modulation[-<span class="hljs-number">1</span>].bias, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.weight, <span class="hljs-number">0</span>)<br>        nn.init.constant_(self.final_layer.linear.bias, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p><strong>Final Layer ——Transformer Decoder</strong></p><pre><code class="hljs">最后一个Dit Block后，需要将image tokens的序列解码为输出噪声以及对角的协方差矩阵的预测结果，且这两个输出的形状都与原始空间输入一致。作者在该环节使用标准的线性解码器，将每个token线性解码为p × p× 2C的张量，其中C是空间输入中到Dit的通道</code></pre><p>数，最后将解码的tokens重新排列到其原始空间布局中，得到预测的噪声和协方差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FinalLayer</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    The final layer of DiT.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size, patch_size, out_channels</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=<span class="hljs-literal">False</span>, eps=<span class="hljs-number">1e-6</span>)<br>        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=<span class="hljs-literal">True</span>)<br>        self.adaLN_modulation = nn.Sequential(<br>            nn.SiLU(),<br>            nn.Linear(hidden_size, <span class="hljs-number">2</span> * hidden_size, bias=<span class="hljs-literal">True</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, c</span>):<br>        shift, scale = self.adaLN_modulation(c).chunk(<span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        x = modulate(self.norm_final(x), shift, scale)<br>        x = self.linear(x)<br>        <span class="hljs-keyword">return</span> x   <span class="hljs-comment">##(N, T, patch_size ** 2 * out_channels)</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">unpatchify</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    x: (N, T, patch_size**2 * C)</span><br><span class="hljs-string">    imgs: (N, H, W, C)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    c = self.out_channels<br>    p = self.x_embedder.patch_size[<span class="hljs-number">0</span>]<br>    h = w = <span class="hljs-built_in">int</span>(x.shape[<span class="hljs-number">1</span>] ** <span class="hljs-number">0.5</span>)<br>    <span class="hljs-keyword">assert</span> h * w == x.shape[<span class="hljs-number">1</span>]<br><br>    x = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], h, w, p, p, c))<br>    x = torch.einsum(<span class="hljs-string">&#x27;nhwpqc-&gt;nchpwq&#x27;</span>, x)<br>    imgs = x.reshape(shape=(x.shape[<span class="hljs-number">0</span>], c, h * p, h * p))<br>    <span class="hljs-keyword">return</span> imgs<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self.out_channels = in_channels * <span class="hljs-number">2</span> <span class="hljs-keyword">if</span> learn_sigma <span class="hljs-keyword">else</span> in_channels<br></code></pre></td></tr></table></figure><p>说明了如果预测协方差矩阵的化，out_channels为2c，即第一部分c为噪声，第二部分c为预测的协方差矩阵</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型无监督训练</title>
    <link href="/2024/12/03/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83/"/>
    <url>/2024/12/03/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%A0%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<p>无监督训练是机器学习的一种方式，它不依赖于带标签的数据来训练模型，而是通过从未标注的数据中发现潜在的结构或模式。对于语言模型，无监督学习通常意味着让模型通过大量的文本数据自我学习，从而能够理解语言的结构、上下文、语法规则。</p><h2 id="无监督训练方法">无监督训练方法</h2><ul><li>自编码器：通过将输入映射到潜在空间，再从潜在空间恢复回输入来训练，学习数据的低维表示</li><li>聚类：聚类算法视图将数据集分城若干类别或簇，模型通过这些簇来发现数据中的结构</li><li>生成模型：生成式模型视图学习数据的分布，并从中生成新的数据</li></ul><h2 id="无监督训练在llm中的应用">无监督训练在LLM中的应用</h2><p>LLM通常采用的无监督训练方式是通过<strong>自回归训练</strong>（autoregressive training）或者<strong>掩码语言建模</strong>（masked language modeling）来训练。</p><ul><li>自回归训练语言建模：模型的目标是预测一个句子中的每个单词的下一个单词，对于大量未标注的文本数据，模型通过这种方式逐步学习语言的结构和上下文信息</li><li>掩码语言建模：通常用于Bert类模型，句子随机掩盖掉一些词，模型的任务是根据上下文信息预测被掩盖的词。例如，在句子“The cat sat on the [MASK]”中，模型需要根据上下文预测出被掩盖的词“mat”</li></ul><p>2）从未标注的文本中学习表示</p><p>无监督训练的另一个关键应用是让模型通过海量的文本数据学习到词语的表示，如词嵌入是通过无监督方法生成的，它能够将单词映射到高维空间，使得语义相近的词在该空间中更为接近。</p><p>3）迁移学习</p><p>无监督训练往往是LLM的预训练阶段，而在实际应用时，模型可能会根据特定任务进行微调。</p><p>无监督训练在大规模语言模型（LLM）中的应用是至关重要的，它帮助模型从大量未标注的文本中学习语言的结构、上下文以及潜在的语义关系。通过无监督学习，LLM能够掌握丰富的语言知识，并为下游任务提供强大的支持。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型微调方式</title>
    <link href="/2024/12/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F/"/>
    <url>/2024/12/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="全量微调-full-fine-tuning">全量微调 Full Fine-tuning</h1><p>原理：在预训练的大型模型基础上，对模型的所有层和参数进行调整，使其适应特定任务。在这个过程中，模型会根据特定任务的数据重新学习和更新所有的权重参数，以达到更好地完成该任务的目的。</p><p>优点：可以充分利用预训练模型的通用特征，较好地适应特定任务。</p><p>缺点：计算成本高</p><h1 id="基于适配器adapter的微调">基于适配器（Adapter）的微调：</h1><p>原理：在预训练模型的每一层或某些层中添加适配器模块，微调时冻结预训练模型主体。由适配器模块学习特定下游任务的知识。</p><p>适配器由两个前馈子层组成，第一个将模型的输出作为输入，投影到较小维度，第二个将其还原到原始输入维度作为输出</p><p>优点：只需要训练少量的特定于任务的参数，降低了训练的计算成本和存储需求，预训练模型的主体被冻结，保留预训练模型的大部分知识，能快速适应新的任务。</p><p>缺点：增加了模型的复杂性，可能影响模型的推理速度，且适配器模块的设计要求较高</p><h1 id="基于低秩适应lora的微调">基于低秩适应（LORA）的微调</h1><p>Low-Rank Adaptation of Large Language Models</p><p>原理：冻结预训练模型的矩阵参数，并引入额外的低秩矩阵代替模型权重的变化</p><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241122143906738.png" alt="image-20241122143906738" /><figcaption aria-hidden="true">image-20241122143906738</figcaption></figure><p><span class="math display">\[h=W_0 x+\Delta Wx=W_0 x+BAx\]</span></p><p>学习的是BA矩阵，来量化权重的差异<span class="math inline">\(\Delta W\)</span></p><h1 id="p-tuning">P-Tuning</h1><p>为了解决人工设计的Prompt问题： GPT Understands,TOO</p><ul><li>大模型的Prompt构造方式严重影响下游任务的效果，如GPT3采用人工构造的模板做上下文学习，模板的变化较敏感，加减词或变动位置会造成较大变化</li><li>自动化搜索模板工作成本较高，离散化token搜索出的结果可能并不最优，性能不稳定</li></ul><p>P-Tuning将提示Prompt转化为可学习的Embedding层，并使用MLP+LSTM的方式对Prompt Embedding进行一层处理。</p><ul><li>P-Tuning 设计了一种连续可微的virtual token，将Prompt转换为可学习的Embedding层，使用MLP+LSTM的方式对Prompt Embedding进行一层处理。</li><li>经过预训练的LM的词嵌入变得高度离散，若随机初始化virtual token，容易优化到局部最优，作者通过实验发现用一个prompt encoder编码收敛更快，效果更好。"virtual token"指的是一种用于微调语言模型的特殊标记。</li></ul><figure><img src="https://jc2001-1307981922.cos.ap-beijing.myqcloud.com/image-20241122155804651.png" alt="image-20241122155804651" /><figcaption aria-hidden="true">image-20241122155804651</figcaption></figure><h1 id="qloraquantized-lora">QLORA（Quantized LoRA）</h1><h3 id="模型量化">模型量化</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/10/10/hello-world/"/>
    <url>/2023/10/10/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
